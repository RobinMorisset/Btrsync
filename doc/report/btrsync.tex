\documentclass[11pt]{llncs}

\def\makeitbig{%
\setlength{\textwidth}{15.9cm}%
\setlength{\oddsidemargin}{.01cm}%
\setlength{\evensidemargin}{.01cm}%
\setlength{\textheight}{21.5cm}%
\setlength{\topmargin}{-.25cm}%
\setlength{\headheight}{.7cm}%
\leftmargini 20pt     \leftmarginii 20pt%
\leftmarginiii 20pt   \leftmarginiv 20pt%
\leftmarginv 12pt     \leftmarginvi 12pt%
\pagestyle{myheadings}}%

\makeitbig

\usepackage{algorithmicx}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsfonts, amssymb, graphicx, rotating, epsfig}
\usepackage{verbatim,algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{url,tikz,tabularx,multirow,xspace,booktabs}
\usetikzlibrary{trees,arrows,chains,matrix,positioning,scopes}

\newcommand*\Let[2]{\State #1 $\gets$ #2}
%\algrenewcommand\alglinenumber[1]{
%    {\sf\footnotesize\addfontfeatures{Colour=888888,Numbers=Monospaced}#1}}


\newcommand{\sig}{{\sf $($Gene\-rate, Sign, Verify$)$} }
\newcommand{\ignore}[1]{}
\newcommand{\btr}{{\tt btrsync}}
\newcommand{\rsy}{{\tt rsync}}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\Cov}[0]{\mbox{Cov}}
\newcommand{\Var}[0]{\mbox{Var}}
\newcommand{\xor}[0]{\oplus}
\newcommand{\rmu}[0]{\mbox{RM}}
\newcommand{\Prob}[1]{{\Pr\left[\,{#1}\,\right]}}
\newcommand{\EE}[1]{{\mathbb{E}\left[{#1}\right]}}
\newcommand{\Oapp}{\ensuremath{\tilde{O}}}

\newcommand{\btrsync}{\texttt{btrsync}\xspace}
\newcommand{\rsync}{\texttt{rsync}\xspace}

\newcommand{\comm}[1]{\marginpar{%
\vskip-\baselineskip %raise the marginpar a bit
\raggedright\footnotesize
\itshape\hrule\smallskip#1\par\smallskip\hrule}}
\setlength{\marginparwidth}{2cm}

\usepackage{hyperref, geometry}

\begin{document}

\title{From Rational Number Reconstruction to Set Reconciliation}

\author{Antoine Amarilli \and Fabrice Ben Hamouda \and Florian Bourse \and\\
Robin Morisset \and David Naccache \and Pablo Rauzy}

\institute{
\'{E}cole normale sup\'{e}rieure, D\'{e}partement d'informatique \\
   45, rue d'Ulm, {\sc f}-75230, Paris Cedex 05, France.\\
   \email{surname.name@ens.fr} (except for \email{fabrice.ben.hamouda@ens.fr})
}

\maketitle

\comm{Si cet abstract vous convient, il faudrait en reprendre des bouts dans l'introduction.}
\begin{abstract}
  This work revisits {\sl set reconciliation}, a problem consisting in synchronizing two multisets of fixed-size values while minimizing the amount of data transmitted. We propose a new number theoretic reconciliation protocol called ``Divide \& Factor'' ({\sc d\&f}) which achieves optimal asymptotic transmission complexity like prior proposals. We then study the problem of synchronizing sets of variable-size files, and describe how constant-factor improvements can be achieved through the use of hashing with a carefully chosen hash size (balancing the quantity of data transferred and the risk of collisions). We show how this process can be applied to synchronize file hierarchies, taking into account the location of files. We describe \btrsync, our open-source implementation of the protocol, and benchmark it against the popular software \rsync to demonstrate that \btrsync uses more CPU time but transmits less data.
\end{abstract}

\section{Introduction}
This work revisits {\sl set reconciliation}, a problem consisting in
synchronizing two multisets while minimizing the amount of data transmitted. Set
reconciliation arises in many practical situations, the most typical of which is
certainly incremental backups performed over a slow network link.\smallskip

\comm{Est-ce que c'est vrai qu'on apporte quelque chose de nouveau dans le cas o√π c'est des fichiers de taille non fixe ?}
Several efficient and elegant solutions are known to achieve set reconciliation of multisets containing atomic elements of a fixed size. For instance, \cite{PSRec} manages to perform set reconciliation using a bandwidth which is linear in the size of the symmetric difference of the multisets multiplied by the size of the elements, which is optimal in this setting. We refer the reader to~\cite{PSRec,Mins1,Whats} (to quote a few references) for more on this problem's history and its existing solutions.\smallskip

However, in the case where the elements to be synchronized can be very large (e.g., files during a backup), we must use checksums to identify the differing files before transferring them, and the question of the size of the checksum to use is non-trivial. In this article, we propose a new reconciliation protocol called ``Divide \& Factor'' ({\sc d\&f}) based on number theory. In terms of asymptotic transmission complexity, the proposed procedure reaches optimality as well. In addition, the new protocols offer a very interesting gamut of parameter trade-offs. We provide an analysis of the protocol's complexity in terms of transmission and computation, as well as a probabilistic analysis of the possible choices of checksum sizes; we also provide an implementation of the protocol and experimental results.\smallskip

This paper is structured as follows: Section \ref{basic} presents a basic version of the proposed protocol. This basic version suffers from two limitations: it works only if the number of differences to reconcile is bound and it may fail leave the synchronized party in an erroneous state. Failure avoidance is overcome in section \ref{reco} and an extension to arbitrary numbers of differences is given in section \ref{insuf}. The protocol's transmission complexity is treated in section \ref{trans}. Section \ref{trans} also introduces two transmission optimizations and analyzes them in detail. Section \ref{comp} analyzes the computational complexities of the proposed protocols and \ref{program} reports practical experiments and benchmarks against the popular software \rsync.\smallskip

\section{``Divide \& Factor'' Set Reconciliation}
\label{dandf}

\subsection{Problem Definition and Notations}

\underline{O}scar possesses an \underline{o}ld version of a directory $\mathfrak{D}$ that he wishes to update. \underline{N}eil has the \underline{n}ew, up-to-date version $\mathfrak{D}'$: $\mathfrak{D}$ and
$\mathfrak{D}'$ can differ both in their files and in their tree structures. Oscar wishes to obtain $\mathfrak{D}'$ but {\sl exchange as little data as possible} during the synchronization process.\smallskip

To tackle this problem we separate the {\sl ``what''} from the {\sl ``where''} by considering files as a tuple of their location and content. In other words, we will first synchronize all the file contents and then move files to the adequate location. We consider that $\mathfrak{D}$ is a multiset of files which we denote as $\mathfrak{F}=\{F_0,\ldots,F_{n}\}$, and likewise represent $\mathfrak{D'}$ as $\mathfrak{F}'=\{F'_0,\ldots,F'_{n'}\}$.\smallskip

Let $t_0$ be the number of discrepancies between $\mathfrak{F}$ and $\mathfrak{F}'$ that Oscar wishes to learn, {\sl i.e.} the symmetric difference of $\mathfrak{F}$ and $\mathfrak{F}'$:

$$t_0=\#\mathfrak{F}+\#\mathfrak{F}'-2 \#\left(\mathfrak{F} \bigcap \mathfrak{F}'\right)=\#\left(\mathfrak{F}\bigcup\mathfrak{F}'\right)-\#\left(\mathfrak{F}\bigcap\mathfrak{F}'\right)$$

Given a file $F$, we denote by $\mbox{{\tt Hash}}(F)$ its image by a collision-resistant hash function such as {\sc sha}-1. Let $\mbox{{\tt HashPrime}}(F)$\footnote{The design of \mbox{{\tt HashPrime}} is addressed in Appendix \ref{sec:hashprime}.} be a function hashing files (uniformly) into primes smaller than $2^u$ for some $u\in \mathbb{N}$. Define the shorthand notations: $h_i=\mbox{{\tt HashPrime}}(F_i)$ and $h'_i=\mbox{{\tt HashPrime}}(F'_i)$.\smallskip

\subsection{Description of the Basic Exchanges}
\label{basic}

The number of differences $t_0$ is unknown to Oscar and Neil. However, for the time being, we will assume that $t_0$ is smaller than some $t$ and attempt to perform
synchronization. If $t_0\leq t$, synchronization will succeed; if $t_0 < t$ the parties will transmit more information later to complete the synchronization, as explained in section \ref{insuf}.

We generate a prime $p$ such that:

\begin{equation}
\label{equp}
2^{2ut+1} \leq p < 2^{2ut+2}
\end{equation}

Given $\mathfrak{F}$, Oscar generates and sends to Neil the redundancy:

$$
c=\prod_{F_i\in \mathfrak{F}} \mbox{{\tt HashPrime}}(F_i)=\prod_{i=1}^n h_i \bmod p
$$

Neil computes:\smallskip

$$c'=\prod_{F'_i\in \mathfrak{F'}} \mbox{{\tt HashPrime}}(F'_i)=\prod_{i=1}^{n'} h'_i \bmod p{~~~\mbox{and}~~~}s=\frac{c'}{c} \bmod p$$

Using~\cite{vallee} the integer $s$ can be written as:
$$s=\frac{a}{b} \bmod p{\mbox{~where the~}G_i\mbox{~denote files and~}}
\left\{
\begin{array}{lcr}
a & =&  \prod\limits_{G_i \in \mathfrak{F}'\wedge G_i \not\in\mathfrak{F}} \mbox{{\tt HashPrime}}(G_i) \\
\\
b & = & \prod\limits_{G_i \not\in\mathfrak{F}' \wedge G_i \in\mathfrak{F}} \mbox{{\tt HashPrime}}(G_i)
\end{array}
\right.
$$

Note that if our assumption $t_0 \leq t$ is correct, $\mathfrak{F}$ and $\mathfrak{F}'$ differ by at most $t$ elements and $a$ and $b$ are strictly less than $2^{ut}$. The problem of recovering $a$ and $b$ from $s$ efficiently is known as {\sl Rational Number Reconstruction}~\cite{pan2004rational,wang2003acceleration}; theorem \ref{theo} (see~\cite{cryptorational}) guarantees that it can be solved in this setting.

\begin{theorem}
\label{theo}
Let $a,b \in {\mathbb Z}$ such that $-A \leq a \leq A$ and $0<b \leq B$. Let $p>2AB$ be a prime and $s=a b^{-1} \bmod p$. Then $a,b$ can be recovered from $A,B,s,p$ in polynomial time.
\end{theorem}

Taking $A=B=2^{ut}-1$, Equation \eqref{equp} implies that $2AB<p$. Moreover, $0 \leq a \leq A$ and $0 <b \leq B$. Thus Oscar can recover $a$ and $b$ from $s$ in polynomial time: a possible option is o use Gauss' algorithm for finding the shortest vector in a bi-dimensional lattice~\cite{vallee}.
By testing the divisibility of $a$ and $b$ by the $h_i$ and the $h'_i$, Neil and Oscar can attempt to identify the discrepancies between $\mathfrak{F}$ and $\mathfrak{F}'$ and settle them.\smallskip

\begin{figure}
\begin{center}
\begin{tabular}{|lcl|}\hline
~~{\bf Oscar}                       &                                                      &   {\bf Neil}~\\
~~compute $c$&                                                      &\\
                                   &~~{{\LARGE $\stackrel{c}{\longrightarrow}$}}~~        &   \\
                                   &                                                      &compute $a,b$~\\
                                   &                                                      &if $a$ doesn't factor as a product of $h'_i$s~~\\
                                   &                                                      &~~~~then output $\bot_{\mbox{{\tiny {\sf bandwidth}}},1}$ and halt~~\\
                                   &                                                      &$\mathfrak{S}\leftarrow\{F'_i \mbox{~s.t.~} a \bmod h'_i =0\}$~~\\
                                   &~~{\LARGE $\stackrel{\mathfrak{S},b}{\longleftarrow}$}&\\
~~if $b$ doesn't factor as a product of $h_i$'s&&\\
~~~~~~then output $\bot_{\mbox{{\tiny {\sf bandwidth}}},2}$ and halt &&\\
~~delete files s.t. $b \bmod h_i =0$&                                                      &\\
~~add $\mathfrak{S}$ to the disk    &                                                      &\\\hline
\end{tabular}
\end{center}
\caption{Basic Protocol.}\label{fig:one}
\end{figure}

The formal description of the protocol is given in Figure \ref{fig:one}. The ``output $\bot_{\mbox{{\tiny {\sf bandwidth}}},\square}$ '' protocol interruptions will:

\begin{itemize}
\item never occur if the assumption $t_0 \leq t$ holds.

\item occur with high probability if $t_0 > t$. Indeed, for a potential $\bot_{\mbox{{\tiny {\sf bandwidth}}},1}$ to be overlooked, the $ut$-bit number $a$ must perfectly factor over a set of $n$ primes of size $u$. If we assume that $a$ is ``random'', the probability $\gamma$ that $a$ is divisible by some $h_i$ is essentially $\gamma \sim 1/h_i \sim 2^{-u}$, the probability that $a$ is divisible by exactly $t$ digests is:
$$\alpha=\binom{n}{t} \gamma^t (1 - \gamma)^{n - t} \sim \binom{n}{t} 2^{-u t} (1 - 2^{-u})^{n - t}$$ and the probability that the protocol does not terminate by a $\bot_{\mbox{{\tiny {\sf bandwidth}}},\square}$ when $t_0 > t$ is $\sim\alpha^2$.
\end{itemize}

The very existence of $\bot_{\mbox{{\tiny {\sf bandwidth}}},\square}$'s is annoying for two reasons:
\begin{itemize}
\item A file synchronization procedure that works {\sl only} for a limited number of differences is not really useful in practice. Thus, section \ref{insuf} explains how to extend the protocol to perform the synchronization even when the number of differences $t_0$ exceeds the initial estimation $t$.\smallskip
\item If, by sheer bad luck, both $\bot_{\mbox{{\tiny {\sf
  bandwidth}}},\square}$'s went undetected (double accidental factorization) the
  Basic Protocol (Fig. \ref{fig:one}) may leave Oscar in an inconsistent state.
\end{itemize}

Double accidental factorization is not only possible source of inconsistent states: as we did not specifically require $\mbox{{\tt HashPrime}}$ to be collision-resistant, the events

$$
\begin{array}{lll}
{
\bot_{\mbox{{\tiny {\sf collision}}},1}=\left\{
\begin{array}{l}
h'_i = h'_j \mbox{~for~}i\neq j\\
\\
a \bmod h_i =0
\end{array}
\right.
}&\mbox{~~~and/or~~~}&{
\bot_{\mbox{{\tiny {\sf collision}}},2}=\left\{
\begin{array}{l}
h_i = h_j \mbox{~for~}i\neq j\\
\\
b \bmod h'_i =0
\end{array}
\right.}\\
\end{array}
$$

will cause Neil to send wrong files in $\mathfrak{S}$ ($\bot_{\mbox{{\tiny {\sf collision}}},1}$) and/or have Oscar unduely delete files owned by Neil ($\bot_{\mbox{{\tiny {\sf collision}}},2}$).\smallskip

Inconsistent states may hence stem from three events:

\begin{center}
\begin{tabular}{llll}
$\bullet$~~&\multicolumn{3}{l}{accidental double factorization of $a$ and/or $b$ when $t_0 > t$ (probability $\alpha^2$)}\\
$\bullet$~~&$\bot_{\mbox{{\tiny {\sf collision}}},1}$   &$=$& collisions within the set $\{h'_i\}$\\
$\bullet$~~&$\bot_{\mbox{{\tiny {\sf collision}}},2}$ &$=$& collisions within the set $\{h_i\}$\\
\end{tabular}\smallskip
\end{center}

Section \ref{reco} explains how protect the protocol from all inconsistent events at once.

\subsection{Avoiding Inconsistency}
\label{reco}

The Basic Protocol of Figure \ref{fig:one} is fully deterministic. Hence if any sort of trouble occurs, repeating the protocol will be of no help. We modify the protocol as follows:

\begin{itemize}
\item Let $H\leftarrow\mbox{{\tt Hash}}(\mathfrak{F}')$
\item Replace $\mbox{{\tt HashPrime}}(F)$ by a diversified $\hbar_k(F)=\mbox{{\tt HashPrime}}(k|F)$.\smallskip
\item Define the shorthand notations: $\hbar_{k,i}=\hbar_k(F_i)$ and $\hbar'_{k,i}=\hbar_k(F'_i)$.\smallskip
\item Let $\mbox{{\sf StepProtocol}}(k)$ denote the sub-protocol shown in Figure \ref{fig:step}.
\item Use the protocol of Figure \ref{fig:itera} as a fully functional reconciliation protocol for $t_0 \leq t$.
\end{itemize}

\begin{figure}
\begin{center}
\begin{tabular}{|lcl|}\hline
~~{\bf Oscar}                       &                                                      & {\bf Neil}~\\
                                   &                                                       &if $a$ doesn't factor as a product of $\hbar'_{k,i}$s~~\\
                                   &                                                       &~~~~then output $\bot_{\mbox{{\tiny {\sf bandwidth}}},1}$ and halt~~\\
                                   &                                                       &$\mathfrak{S}\leftarrow\{F'_i \mbox{~s.t.~} a \bmod \hbar'_{k,i} =0\}$~~\\
~~                                 &                                                       &if there are collisions in $\mathfrak{S}$\\
                                   &                                                       &~~~~then output $\bot_{\mbox{{\tiny {\sf collision}}},1}$ and halt~~\\
                                   &~~{\LARGE $\stackrel{\mathfrak{S},b}{\longleftarrow}$} &\\
~~if $b$ doesn't factor as a product of $\hbar_{k,i}$'s&&\\
~~~~~~then output $\bot_{\mbox{{\tiny {\sf bandwidth}}},2}$ and halt &&\\
~~$\mathfrak{A}\leftarrow\{F_i \mbox{~s.t.~} b \bmod \hbar_{k,i} =0\}$ &&\\
~~if there are collisions in $\mathfrak{A}$ &                                   & \\
~~~~~~then output $\bot_{\mbox{{\tiny {\sf collision}}},2}$ and halt~~&                      &~~\\
~~if $H \neq \mbox{{\tt Hash}}(\mathfrak{F}\bigcup\mathfrak{S} - \mathfrak{A})$ &                                                      &\\
~~~~~~then output $\bot_{\mbox{{\tiny {\sf bandwidth}}},3}$ and halt &&\\
~~add $\mathfrak{S}$ to the disk and erase $\mathfrak{A}$ from the disk &                                                      &\\
~~return {\sf success} &                                                      &\\\hline
\end{tabular}
\end{center}
\caption{$\mbox{{\sf StepProtocol}}(k)$.}\label{fig:step}
\end{figure}

\subsubsection{Note:} To avoid transmitting the (potentially very voluminous) $\mathfrak{S}$ during {\sf StepProtocol} before knowing if one of the errors $\bot_{\mbox{{\tiny {\sf bandwidth}}},2},\bot_{\mbox{{\tiny {\sf bandwidth}}},3},\bot_{\mbox{{\tiny {\sf collision}}},2}$ will occur, Neil may transmit $$\mathfrak{S}'=\{\mbox{{\tt Hash}}(F'_i),~F'_i\in \mathfrak{S}\}$$ instead of $\mathfrak{S}$ and send $\mathfrak{S}$ only after successfully passing the $\bot_{\mbox{{\tiny {\sf bandwidth}}},3}$ test. The definition of $H$ must be changed accordingly to 

$$H=\mbox{{\tt Hash}}(\{\mbox{{\tt Hash}}(F'_i),~F'_i\in \mathfrak{F}'\})$$

\begin{figure}
\begin{center}
\begin{tabular}{|lcl|}\hline
~~{\bf Oscar}                      &                                                      &   {\bf Neil}~\\
~~                                 &                                                      & compute $H\leftarrow\mbox{{\tt Hash}}(\mathfrak{F}')$\\
                                   &~~{{\LARGE $\stackrel{H}{\longleftarrow}$}}~~   &   \\
~~compute $c$&                                                                             &\\
                                   &~~{{\LARGE $\stackrel{c}{\longrightarrow}$}}~~         &   \\
                                   &                                                       &compute $a,b$~\\
                                   &                                                      &$k\leftarrow 1$~\\
                                   &~~{{\LARGE $\stackrel{\mbox{{\small{\sf StepProtocol}}}(k)}{\longleftarrow}$}}~~   &while $\mbox{{\sf StepProtocol}}(k)=\bot_{\mbox{{\tiny {\sf collision}}},\square}$~~\\
                                   &                                                      &~~~~$k\leftarrow k+1$~\\\hline
\end{tabular}
\end{center}
\caption{Fully Functional Protocol for $t_0 \leq t$.}\label{fig:itera}
\end{figure}

\subsection{Handling a High Number of Differences}
\label{insuf}

To extend the protocol to an arbitrary $t_0$, Oscar and Neil agree on an infinite set of primes $p_1,p_2,\ldots$ As long as the protocol fails with a $\bot_{\mbox{{\tiny {\sf bandwidth}}},\square}$ status, Neil will keep accumulating information about the difference between $\mathfrak{F}$ and $\mathfrak{F}'$ as shown in Appendix \ref{sec:extended}. Note that no information is lost and that the transmitted modular knowledge about the difference adds up until it reaches a threshold sufficient to reconcile $\mathfrak{F}$ and $\mathfrak{F}'$. \smallskip

All $\bot$ treatments were removed from Appendix \ref{sec:extended} for the sake of clarity (these can be very easily added by modifying Appendix \ref{sec:extended} {\sl mutatis mutandis}). In essence, the rules are: add information modulo a new $p_i$ whenever the protocol fails with a $\bot_{\mbox{{\tiny {\sf bandwidth}}},\square}$ and increment $k$ whenever the protocol fails with a $\bot_{\mbox{{\tiny {\sf collision}}},\square}$.\smallskip

A typical execution sequence is thus expected to be something like:

$$\bot_{\mbox{{\tiny {\sf bandwidth}}},1},\bot_{\mbox{{\tiny {\sf bandwidth}}},1},\bot_{\mbox{{\tiny {\sf bandwidth}}},1},\bot_{\mbox{{\tiny {\sf bandwidth}}},1},\bot_{\mbox{{\tiny {\sf collision}}},1},\bot_{\mbox{{\tiny {\sf collision}}},1},\mbox{{\sf success}}$$

\section{Transmission Complexity}
\label{trans}
This section explores two strategies for reducing the size of $p$ and hence improving transmission by {\sl constant factors} (from an asymptotic communication standpoint, improvements cannot be expected as the protocol already transmits information proportional to $t_0$, the difference to settle). Excluding the core information $\mathfrak{S}$ and assuming that no $\bot_{\mbox{{\tiny {\sf collision}}},\square}$ events occurred, the transmission complexity of the protocol of Appendix \ref{sec:extended} is:

$$
\lambda \log (\max_{i=1}^\lambda c_i) + \log b \leq \lambda \log (\max_{i=1}^\lambda p_i) + \frac12 \log \prod_{i=1}^\lambda p_i \leq 3\lambda (ut_0+1)=O(\lambda ut_0)=O(ut)
$$

Where $\lambda=t/t_0$ is the number of rounds required to complete the protocol. As we have no control over $t$, decreasing $u$ is the main natural optimization option. We will get back to this later on in this paper (section \ref{shortu}).

\subsection{Probabilistic Decoding: Reducing $p$}

Generate a prime $p$ about twice shorter than the $p$ recommended in section \ref{basic}, namely:
\begin{equation}
\label{eqnewp}
2^{u(t+1)}<p \leq 2^{u(t+1)+1}
\end{equation}

Let $\eta=\max(n,n')$. The new redundancy $c$ is calculated as previously and is hence also approximately twice smaller. Namely:

$$s=\frac{a}{b} \bmod p \mbox{~and~}
\left\{
\begin{array}{lcr}
a & =&  \prod\limits_{G_i \in \mathfrak{F}'\wedge G_i \not\in\mathfrak{F}} \mbox{{\tt HashPrime}}(G_i) \\
\\
b & = & \prod\limits_{G_i \not\in\mathfrak{F}' \wedge G_i \in\mathfrak{F}} \mbox{{\tt HashPrime}}(G_i)
\end{array}
\right.
$$

and since there are at most $t$ differences, we must have:
\begin{equation}
\label{eqab}
a b \leq 2^{ut}
\end{equation}

By opposition to section \ref{basic} we do not have a fixed bound for $a$ and $b$ anymore; Equation \eqref{eqab} only provides a bound for the {\sl product} $a b$. Therefore, we define a sequence of at most $t+1$ couples of bounds:

$$\left(A_i,B_i\right)=\left(2^{(u+1)i},\left\lfloor \frac{p-1}{2^{(u+1)i+1}} \right\rfloor\right)\mbox{~~where~~}B_i>1\mbox{~~and~~}\forall i>0,~2 A_i B_i<p$$

Equations (\ref{eqnewp}) and (\ref{eqab}) imply that there must exist at least one index $i$ such that $0 \leq a \leq A_i$ and $0 <b \leq B_i$. Then using Theorem \ref{theo}, given $(A_i,B_i,p,s)$ one can recover $(a,b)$, and hence the difference between $\mathfrak{F}$ and $\mathfrak{F}'$.\smallskip

The problem is that (unlike section \ref{basic}) we have no guarantee that such an $(a,b)$ is unique. Namely, we could (in theory) stumble over an $(a',b')\neq (a,b)$ satisfying (\ref{eqab}) for some index $i' \neq i$. We conjecture that such failures happen with negligible probability (that we do not try to estimate here) when $u$ is large enough, but this makes the modified protocol heuristic only. If failures never occur, this variant will roughly halve the quantity of transmitted bits with respect to section \ref{basic}.\smallskip

\subsection{The File Laundry: Reducing $u$}
\label{shortu}
What happens if we brutally shorten $u$ in the basic Divide \& Factor protocol?\smallskip

As expected by the birthday paradox, we should start seeing collisions. Let us analyze the statistics governing the appearance of collisions.

Consider $\mbox{{\tt HashPrime}}$ as a random function from $\{0,1\}^*$ to $\{0,\dots,2^u-1\}$. Let $X_i$ be the random variable:

$$
X_i =
\left\{
\begin{array}{lcl}
1 & ~~~~&  \mbox{if file $F_i$ collides with another file.}\\
\\
0 & ~~~~&  \mbox{otherwise.}
\end{array}
\right.
$$

Clearly, we have $\Prob{X_i = 1} \le \frac{\eta -1}{2^u}$.
The average number of colliding files is hence:
\[ \EE{\sum_{i=0}^{\eta-1} X_i} \le \sum_{i=0}^{\eta-1} \frac{\eta -1}{2^u} = \frac{\eta (\eta - 1)}{2^u} \]

For instance, for $\eta=10^6$ files and 32-bit digests, the expected number of colliding files is less than $233$.\smallskip

However, it is important to note that a collision can only yield a {\sl false positive}, and never a {\sl false negative}. In other words, while a collision may obliviate a difference\footnote{{\sl e.g.} make the parties blind to the difference between {\tt index.htm} and {\tt iexplore.exe}.} a collision will never create a nonexistent difference {\sl ex nihilo}.\smallskip

Thus, it suffices to replace $\mbox{{\tt HashPrime}}(F)$ by a diversified $\hbar_k(F)=\mbox{{\tt HashPrime}}(k|F)$ to quickly filter-out file differences by repeating the protocol for $k=1,2,\ldots$ At each iteration the parties will detect new files and new deletions, fix these and ``launder'' again the remaining multisets.\smallskip

Assume that the diversified $\hbar_k(F)$'s are random and independent. To understand why the probability that a stubborn file persists colliding decreases exponentially with the number of iterations $k$, assume that $\eta$ remains invariant between iterations and define the following random variables:\smallskip

$$
\begin{array}{rcl}
X^{\ell}_i & = &
\left\{
\begin{array}{lcl}
1 & ~~~~&  \mbox{if file $F_i$ collides with another file during iteration $\ell$.}\\
\\
0 & ~~~~&  \mbox{otherwise.}
\end{array}
\right.\\
\\
Y_i = \prod_{\ell=1}^k X^{\ell}_i & = &
\left\{
\begin{array}{lcl}
1 & ~~~~&  \mbox{if file $F_i$ collides with another file during the $k$ first protocol iterations.}\\
\\
0 & ~~~~&  \mbox{otherwise.}
\end{array}
\right.
\end{array}$$

\comm{Fabrice je ne vois pas la difference entre $X^{\ell}_i $ dans cette section et $Z^\ell_i $ dans la suivante. Peux-tu preciser STP}

By independence, we have:

\[
  \Prob{Y_i = 1} = \prod_{\ell=1}^k \Prob{X^{\ell}_i = 1} = \Prob{X^1_i = 1} \dots \Prob{X^k_i = 1} \le \left( \frac{\eta -1}{2^u} \right)^k
\]

Therefore the average number of colliding files is:

\[
 \EE{\sum_{i=0}^{\eta-1} Y_i} \le \sum_{i=0}^{\eta-1} \left( \frac{\eta -1}{2^u} \right)^k =  \eta \left(\frac{\eta - 1}{2^u}\right)^k
\]

And the probability that at least one false positive will survive $k$ rounds is:

\[
\epsilon_k \le \eta \left(\frac{\eta - 1}{2^u}\right)^k
\]

For the previously considered instance\footnote{$\eta=10^6$,$u=32$.} we get $\epsilon_2 \le 5.43\%$ and $\epsilon_3 \le 2 \cdot 10^{-3}\%$.

\subsubsection{A more refined (but somewhat technical) analysis.} As mentioned previously, the parties can remove the files confirmed as different during iteration $k$ and work during iteration $k+1$ only with common and colliding files. Now, the only collisions that can fool round $k$, are the collisions of file-pairs $(F_i,F_j)$ such that $F_i$ and $F_j$ have both already collided during {\sl all the previous iterations}\footnote{Note that we \underline{do not} require that $F_i$ and $F_j$ repeatedly collide {\sl which each other}. {\sl e.g.} we may witness during the first round $\hbar_{1}(F_1)=\hbar_{1}(F_2)$, $\hbar_{1}(F_3)=\hbar_{1}(F_4)$ and $\hbar_{1}(F_5)=\hbar_{1}(F_6)$ while during the second round $\hbar_{2}(F_1)=\hbar_2(F_2)$, $\hbar_{2}(F_3)=\hbar_{1}(F_6)$ and $\hbar_{2}(F_5)=\hbar_{2}(F_4)$ as shown in Figure \ref{fig:masked}.}. We call such collisions ``masquerade balls'' ({\sl cf. } Figure \ref{fig:masked}). Define the two random variables:

\begin{align*}
Z^\ell_i &=
\left\{
\begin{array}{lcl}
1 & ~~~~&  \mbox{if $F_i$ participated in masquerade balls during all $\ell$ first protocol iterations.}\\
\\
0 & ~~~~&  \mbox{otherwise.}
\end{array}
\right. \\
X^{\ell}_{i,j} &=
\left\{
\begin{array}{lcl}
1 & ~~~~&  \mbox{if files $F_i$ and $F_j$ collide during iteration $\ell$.}\\
\\
0 & ~~~~&  \mbox{otherwise.}
\end{array}
\right.
\end{align*}

\def\bruijn{file $F_1$,file $F_2$,file $F_3$,file $F_4$,file $F_5$,file $F_6$}

\begin{figure}[t]
\centering
\begin{tikzpicture}[scale=4]
    \foreach \v [count=\n] in \bruijn
    \node[draw=black!30,fill=black!10,rounded corners] (\n) at (180-\n*60:1) {\v};
  \path (1) edge [<->,above,double,thick]             node {round 1} (2)
        (3) edge [<->,above,double,thick,sloped]             node {round 1} (4)
        (5) edge [<->,above,double,thick,sloped]             node {round 1} (6)
        (1) edge [<->,above,bend left]  node {round 2} (2)
        (3) edge [<->,above]             node {round 2} (6)
        (5) edge [<->,above]             node {round 2} (4)
        (6) edge [<->,above,dashed,sloped]  node {round 3} (2)
        (1) edge [<->,above,dashed,sloped]  node {round 3} (3)
        (5) edge [<->,bend left,above,dashed]  node {round 3} (4);
\end{tikzpicture}
\caption{Illustration of three masquerade balls. Each protocol round is materialized by a different type of arrow. Arrows denotes collisions.}\label{fig:masked}
\end{figure}

Set $Z^0_i = 1$ and write $p_\ell = \Prob{Z^{\ell-1}_{i} = 1 \text{ and } Z^{\ell-1}_{j} = 1} $ for all $\ell$ and $i \neq j$.
For $k \ge 1$, we have:
\begin{align*}
\Prob{Z^k_i=1} &= \Prob{\exists j\neq i \text{, } X^k_{i,j} = 1 \text{, } Z^{k-1}_{i} = 1  \text{ and } Z^{\ell-1}_{j} = 1}  \\
&\le \sum_{j=0, j\neq i}^{\eta-1} \Prob{X^{k-1}_{i,j} = 1} \Prob{Z^{k-1}_{i} = 1 \text{ and } Z^{k-1}_{j} = 1}  \\
&\le \frac{\eta-1}{2^u} p_{k-1}
\end{align*}
Furthermore $p_0 = 1$ and
\begin{align*}
p_\ell &= \Prob{X^{\ell}_0 = X^{\ell}_1 \text{, } Z^{\ell}_{0} = 1 \text{ and } Z^{\ell}_{1} = 1}
  + \Prob{X^{\ell}_0 \neq X^{\ell}_1 \text{, } Z^{\ell}_{0} = 1 \text{ and } Z^{\ell}_{1} = 1} \\
&\le \Prob{X^{\ell}_0 = X^{\ell}_1 \text{, } Z^{\ell-1}_{0} = 1 \text{ and } Z^{\ell-1}_{1} = 1} \\
  &\quad+ \sum_{i \ge 2, j \ge 2} \Prob{X^\ell_{0,i} = 1 \text{, } X^\ell_{1,j} = 1 \text{, } Z^{\ell-1}_{0} = 1 \text{ and } Z^{\ell-1}_{1} = 1} \\
&= \Prob{X^{\ell}_0 = X^{\ell}_1} \Prob{Z^{\ell-1}_{0} = 1 \text{ and } Z^{\ell-1}_{1} = 1} \\
  &\quad+ \sum_{i \ge 2, j \ge 2} \Prob{X^\ell_{0,i} = 1} \Prob{X^\ell_{1,j} = 1} \Prob{Z^{\ell-1}_{0} = 1 \text{ and } Z^{\ell-1}_{1} = 1} \\
&\le \frac{1}{2^u} p_{\ell-1} + \frac{(\eta-2)^2}{2^{2u}} p_{\ell-1} = p_{\ell-1}\left(\frac{1}{2^u}  + \frac{(\eta-2)^2}{2^{2u}}\right)
\end{align*}
hence:
\[ p_\ell \le \left( \frac{1}{2^u} + \frac{(\eta-2)^2}{2^{2u}} \right)^\ell, \]
and
\[ \Prob{Z^\ell_i=1} \le \left( \frac{1}{2^u} + \frac{(\eta-2)^2}{2^{2u}} \right)^{k-1} \]
And finally, the survival probability of at least one false positive after $k$ iterations satisfies:
\nopagebreak
\[
\epsilon'_k \le \frac{\eta(\eta-1)}{2^u} \left( \frac{1}{2^u} + \frac{(\eta-2)^2}{2^{2u}} \right)^{k-1}
\]

For $(\eta=10^6,u=32,k=2)$, we get $\epsilon'_2 \le 0.013\%$.\smallskip

\subsubsection{How to select $u$?}
%
For a fixed $k$, $\epsilon'_k$ decreases as $u$ grows. For a fixed $u$, $\epsilon'_k$ also decreases as $k$ grows. Transmission, however, grows with both $u$ (bigger digests) and $k$ (more iterations). We write for the sake of clarity: $\epsilon'_k = \epsilon'_{k,u,\eta}$.\smallskip

Fix $\eta$. Note that the number of bits transmitted per iteration ($\simeq 3ut$), is proportional to $u$. This yields an expected transmission complexity bound $T_{u,\eta}$ such that:\smallskip

\[T_{u,\eta} \propto u \sum_{k=1}^{\infty} k \cdot \epsilon'_{k,u,\eta}=
\frac{u \eta\left(\eta-1\right)}{2^u} \sum_{k=1}^{\infty} k \left( \frac{1}{2^u} + \frac{\left(\eta-2\right)^2}{2^{2u}} \right)^{k-1}=
\frac{u \eta\left(\eta-1\right) 8^u}{\left(2^u-4^u+\left(\eta-2\right)^2\right)^2}\]

Dropping the proportionality factor $\eta\left(\eta-1\right)$, neglecting $2^u \ll 2^{2u}$ and approximating $(\eta-2)\simeq\eta$, we can optimize the function:
\nopagebreak
\[
\phi_\eta(u)=\frac{u \cdot 8^u}{\left(4^u-\eta^2\right)^2}
\]

$\phi_{10^6}(u)$ admits an optimum for $u=19$.

\subsubsection{Note:} The previous analysis is incomplete because of the following approximations:

\begin{itemize}
\item We consider $u$-bit prime digests while $u$-bit strings contain only about $2^u/u$ primes.

\item We used a fixed $u$ in all rounds. Nothing forbids using a different $u_k$ at each iteration, or even fine-tuning the $u_k$s adaptively as a function of the laundry's effect on the progressively reconciliated multisets..

\item Our analysis treats $t$ as a constant, but large $t$ values increase $p$ and hence the number of potential files detected as different per iteration - an effect disregarded {\sl supra}.
\end{itemize}

A different approach is to optimize $t$ and $u$ experimentally, {\sl e.g.} using the open source {\sc d\&f} program \btrsync\ developed by the authors ({\sl cf.} section  \ref{program}).

\subsection{How to Stop a Probabilistic Washing Machine?} We now combine both optimizations and assume that $\ell$ laundry rounds are necessary for completing some given reconciliation task using a half-sized $p$. By opposition to section \ref{basic}, confirming correct protocol termination is now non-trivial.\smallskip

\comm{La d√©finition de $\zeta(u)$ est assez gratuite vu que √ßa n'intervient pas dans la suite.}
We say that a {\sl round failure} occurs whenever a round results in an $(a',b')\neq (a,b)$ satisfying Equation~\eqref{eqab}. Let the round failure probability be some function $\zeta(u)$ (that we did not estimate). If $u$ is kept small (for efficiency reasons), the probability $\left(1-\zeta(u)\right)^{\ell}$ that the protocol will properly terminate may dangerously drift away from one.\smallskip

If $v$ of $\ell+v$ rounds fail, Oscar needs to solve a problem called {\sl Chinese Remaindering With Errors}~\cite{phong}:\smallskip

\begin{problem}{\sl (Chinese Remaindering With Errors Problem: {\sc crwep}).} Given as input integers $v$, $B$ and $\ell+v$ points $(s_1,p_1),\ldots,(s_{\ell+v},p_{\ell+v})\in \mathbb{N}^2$ where the $p_i$'s are coprime, output all numbers $0 \leq s < B$ such that $s \equiv s_i \bmod p_i$ for at least $\ell$ values of $i$.
\end{problem}

We refer the reader to~\cite{phong} for more on this problem, which is beyond our scope. Boneh~\cite{boneh} provides a polynomial-time algorithm for solving the {\sc crwep} under certain conditions satisfied in our setting.\smallskip

But how can we confirm the solution? As mentioned in section \ref{reco}, Neil will send to Oscar $H=\mbox{{\tt Hash}}(\mathfrak{F}')$ as the interaction starts. As long as Oscar's {\sc crwep} resolution will not yield a state matching $H$, the parties will continue the interaction.

\section{Computational Complexity}
\label{comp}
Let $\mu(k)$ be the time required to multiply two $k$-bit numbers\footnote{We assume that $\forall k,k', \mu(k+k') \ge \mu(k) + \mu(k')$.}.
For na\"{i}ve ({\sl i.e.} convolutive) algorithms $\mu(k) = O(k^2)$, but using {\sc fft} multiplication~\cite{schonhage1971schnelle}, $\mu(k) = \Oapp(k)$. {\sc fft} is experimentally faster than convolutive methods starting at $k \sim 10^6$.
The modular division of two $k$-bit numbers and the reduction of $2k$-bit number modulo a $k$-bit number are also known to cost $\Oapp(\mu(k))$~\cite{burnikel1998fast}.
Indeed, in packages such as {\sf gmp}, division and modular reduction run in $\Oapp(k)$.for sufficiently large $k$.\smallskip

Given that $p \sim 2^{ut}$, we obtain the following complexity analysis:

% TODO: si on introduit \mu, alors il faut s'en servir. Si on dit juste que \mu(k) = \Oapp(k) avec la FFT, alors inutile de d√©finir \mu !
% TODO: en fait je pense qu'il faudrait r√©organiser totalement la section : pr√©senter quelles sont les op√©rations √† faire avec un tableau (et un co√ªt na√Øf), pr√©senter un par un les trucs clever qu'on peut faire √† la place, et avoir un autre tableau √† la fin avec les co√ªts sophistiqu√©s
\comm{Je ne comprends pas quelle la diff√©rence entre les colonnes 3 4 et les colonnes 5 6 de ce tableau.}

\begin{table}
  \begin{tabularx}{\textwidth}{lXp{1.2cm}p{2.3cm}p{1.2cm}p{2.3cm}}\toprule
{\bf \hfill Entity \hfill \null} & {\bf \hfill Computation \hfill \null} &  \multicolumn{4}{c}{{\bf Complexity expressed in $\Oapp$ of}} \\\midrule
Both  & generate primes                                                  & $nu^4$  & Rabin-Miller & $n u^3$  & Rabin-Miller             \\
Both  & compute redundancies $c$ and $c'$                                & $n \cdot \mu(u t)$  & na\"{i}ve product & $n u t$  & using {\sc fft}             \\
Oscar & compute $s = c' / c \bmod p$                                     & $\mu(u t)$    & na\"{i}ve inversion& $u t$    & using {\sc fft}               \\
Oscar & find $a,b$ such that $s = a / b \bmod p$                         & $(u t)^2$   & na\"{i}ve ext. {\sc gcd} & $u t$ &  using~\cite{pan2004rational,wang2003acceleration} \\
Both  & factor $a$ (resp. $b$) by modular reductions                     & $n \cdot \mu(u t)$  & na\"{i}ve reduction & $n u t$  & using {\sc fft}             \\
      & {\bf Overwhelming complexity:}                                   &
    \multicolumn{2}{c}{$\max(nu^4,n \cdot \mu(ut),(u t)^2)$}    &
    \multicolumn{2}{c}{$n u \max(t,u^3)$}              \\\bottomrule
  \end{tabularx}
  \caption{Global Protocol Complexity. In practice $\max((u t)^2,n \cdot \mu(ut))=n(u t)^2$ because $(u t)^2 \ll n \cdot \mu(ut)=n(u t)^2$. This boils down to $\max(n(u t)^2, nu^4)=n(u\max(t,u))^2$.}
  \label{tab:workload}
\end{table}

\subsection{Improvements Using Product Trees}

\comm{Indiquer clairement √† quelles lignes du tableau √ßa se r√©f√®re, et si la complexit√© donn√©e dans le tableau prend ces optims en compte.}
The non-overwhelming (but nonetheless important) complexities of the computations of $(c,c')$ and of the factorizations can be even reduced to $\Oapp(\frac{n}{t} \mu(u t))$ using convolutive methods. These complexities can be reduced to $\Oapp(n u)$ with {\sc fft}~\cite{schonhage1971schnelle}. To simplify the presentation, assume that $t=2^\tau$ is a power of two dividing $n$.

The idea is the following: group $h_i$'s by subsets of $t$ elements and compute the product of each such subset in $\mathbb{N}$.
\[ H_j = \prod_{i=j t}^{j t + t - 1} h_i\in\mathbb{N} \]

Each $H_j$ can be computed in $\Oapp(\mu(u t))$ using the standard product tree method described in Algorithm~\ref{alg:prod-tree} (for $j=0$) illustrated in Figure~\ref{fig:prod-tree}.
Thus, all these $\frac{n}{t}$ products can be computed in $\Oapp(\frac{n}{t} \mu(u t))$. We can then compute $c$ by multiplying the $H_j$ modulo $p$, which costs $\Oapp(\frac{n}{t} \mu(u t))$.\smallskip

\begin{figure}[t]
\centering
\centerline{
\begin{turn}{90}
\begin{tikzpicture}[level/.style={sibling distance=60mm/#1}]
\node (z){$\displaystyle \pi=\pi_1=\prod_{i=0}^{t-1} h_i$}
  child {node (a) {$\displaystyle \pi_2=\prod_{i=0}^{t/2-1} h_i$}
    child {node (b) {$\displaystyle \pi_4=\prod_{i=0}^{t/4-1} h_i$}
      child {node {$\vdots$}
        child {node (d) {\begin{turn}{270}{$\displaystyle \pi_t=h_0$}\end{turn}}}
        child {node (e) {\begin{turn}{270}{$\displaystyle \pi_{t+1}=h_1$}\end{turn}}}
      }
      child {node {$\vdots$}}
    }
    child {node (g) {$\displaystyle \pi_5=\prod_{i=t/4}^{2t/4-1} h_i$}
      child {node {$\vdots$}}
      child {node {$\vdots$}}
    }
  }
  child {node (j) {$\displaystyle \pi_3=\prod_{i=t/2}^{2t/2-1} h_i$}
    child {node (k) {$\displaystyle \pi_6=\prod_{i=t/2}^{3t/4-1} h_i$}
      child {node {$\vdots$}}
      child {node {$\vdots$}}
    }
    child {node (l) {$\displaystyle \pi_7=\prod_{i=3t/4}^{4t/4-1} h_i$}
      child {node {$\vdots$}}
      child {node (c){$\vdots$}
        child {node (o) {\begin{turn}{270}{$\displaystyle \pi_{2t-2}=h_{t-2}$}\end{turn}}}
        child {node (p) {\begin{turn}{270}{$\displaystyle \pi_{2t-1}=h_{t-1}$}\end{turn}}
%
%
          child [grow=right] {node (qe) {} edge from parent[draw=none]
            child [grow=right] {node (q) {$2^\tau \times \mu(\frac{ut}{2^\tau}) \le \mu(u t)$} edge from parent[draw=none]
            child [grow=up] {node (r) {$\vdots$} edge from parent[draw=none]
            child [grow=up] {node (s) {$4\times\mu(\frac{u t}4) \le \mu(u t)$} edge from parent[draw=none]
            child [grow=up] {node (t) {$2\times\mu(\frac{u t}2) \le \mu(u t)$} edge from parent[draw=none]
            child [grow=up] {node (u) {$\mu(u t)$} edge from parent[draw=none]}
          }}}
          child [grow=down] {node (v) {$\tau \times \mu(u t) = \Oapp(\mu(u t))$}edge from parent[draw=none]}
            }
          }
        }
    }
  }
};
\path (q) -- (r) node [midway] {+};
\path (s) -- (r) node [midway] {+};
\path (s) -- (t) node [midway] {+};
\draw[->,>=latex,dashed] (l) to (s);
\path (t) -- (u) node [midway] {+};
\draw[->,>=latex,dashed] (z) to (u);
\draw[->,>=latex,dashed] (j) to (t);
\draw[->,>=latex,dashed] (p) to (q);
\path (q) -- (v) node [midway] {$\displaystyle \le$};
\end{tikzpicture}
\end{turn}
}
\caption{Product Tree}\label{fig:prod-tree}
\end{figure}

\begin{algorithm}
\newcommand{\vstart}{\ensuremath{\mathrm{start}}}
\newcommand{\vmid}{\ensuremath{\mathrm{mid}}}
\newcommand{\vend}{\ensuremath{\mathrm{end}}}
\begin{algorithmic}[1]
\Require{the set $h_i$}
\Ensure{$\pi = \pi_1 = \prod_0^{t-1} h_i$, and $\pi_i$ for $i \in \{1,\dots,2t-1\}$ as in Figure~\ref{fig:prod-tree}}
\State $\pi \gets $ array of size $t$
\Function{prodTree}{$i$,$\vstart$,$\vend$}
  \If{$\vstart = \vend$}
    \State \Return $1$
  \ElsIf{$\vstart+1 = \vend$}
    \State \Return $h_{\mbox{\tiny \vstart}}$
  \Else
    \State $\vmid \gets \lfloor \frac{\vstart+\vend}{2} \rfloor$
    \State $\pi_{2i} \gets $\Call{prodTree}{$2i$,$\vstart$,$\vmid$}
    \State $\pi_{2i+1} \gets $\Call{prodTree}{$2i+1$,$\vmid$,$\vend$}
    \State \Return $\pi_{2i} \times \pi_{2i+1}$
  \EndIf
\EndFunction
\State $\pi_1 \gets $\Call{prodTree}{$1,0,t$}
\end{algorithmic}
\caption{Product Tree Algorithm}\label{alg:prod-tree}
\end{algorithm}

\comm{What is the caveat?}
The same technique applies to factorization\footnote{We explain the process with $a$, this is applicable {\sl ne variatur} to $b$ as well.}, but with a slight {\sl caveat}.\smallskip

After computing the tree product, we can compute the residues of $a$ modulo $H_0$.
Then we can compute the residues of $a \bmod{H_0}$ modulo the two children $\pi_2$ and $\pi_3$ of $H_0 = \pi_1$ in the product tree (depicted in Figure~\ref{fig:prod-tree}), and so on. Intuitively, we descend the product tree doing modulo reduction. At the end ({\sl i.e.}, as we reach the leaves), we obtain the residues of $a$ modulo each of the $h_i$ ($i \in \{0,\dots,t-1\}$). This is described in Algorithm \ref{fig:div-prod-tree} and illustrated in Figure \ref{fig:div-prod-tree}.
We can use the same method for the tree product associated to any $H_j$, and the residues of $a$ modulo each of the $h_i$ ($i \in \{jt,\dots,jt+t-1\}$) for any $j$, {\sl i.e.}, $a$ modulo each of the $h_i$ for any $i$.
\comm{Does it make sense to nest $\Oapp$'s?}
Complexity is $\Oapp(\mu(u t))$ for each $j$, which amounts to a total complexity of $\Oapp(\frac{n}{t} \Oapp(\mu(u t)))$.

\begin{figure}[t]
\centering
\centerline{
\begin{turn}{90}
\begin{tikzpicture}[level/.style={sibling distance=60mm/#1}]
\node (z){$\displaystyle a \bmod \pi_1$}
  child {node (a) {$\displaystyle a \bmod \pi_2$}
    child {node (b) {$\displaystyle a \bmod \pi_3$}
      child {node {$\vdots$}
        child {node (d) {\begin{turn}{270}{$\displaystyle a \bmod h_{0}$}\end{turn}}}
        child {node (e) {\begin{turn}{270}{$\displaystyle a \bmod h_{1}$}\end{turn}}}
      }
      child {node {$\vdots$}}
    }
    child {node (g) {$\displaystyle a \bmod \pi_5$}
      child {node {$\vdots$}}
      child {node {$\vdots$}}
    }
  }
  child {node (j) {$\displaystyle a \bmod \pi_3$}
    child {node (k) {$\displaystyle a \bmod \pi_6$}
      child {node {$\vdots$}}
      child {node {$\vdots$}}
    }
    child {node (l) {$\displaystyle a \bmod \pi_7$}
      child {node {$\vdots$}}
      child {node (c){$\vdots$}
        child {node (o) {\begin{turn}{270}{$\displaystyle a \bmod h_{t-2}$}\end{turn}}}
        child {node (p) {\begin{turn}{270}{$\displaystyle a \bmod h_{t-1}$}\end{turn}}
%
%
          child [grow=right] {node (qe) {} edge from parent[draw=none]
            child [grow=right] {node (q) {$2^{\tau} \times O(\mu(\frac{ut}{2^{\tau}}))= O(\mu(u t))$} edge from parent[draw=none]
            child [grow=up] {node (r) {$\vdots$} edge from parent[draw=none]
            child [grow=up] {node (s) {$4 \times O(\mu(\frac{u t}4)) = O(\mu(u t))$} edge from parent[draw=none]
            child [grow=up] {node (t) {$2 \times O(\mu(\frac{u t}2)) = O(\mu(u t))$} edge from parent[draw=none]
            child [grow=up] {node (u) {$1 \times O(\mu(u t))$} edge from parent[draw=none]}
          }}}
          child [grow=down] {node (v) {$\sum_{i=1}^\tau 2^{i} \times O(\mu(\frac{ut}{2^i})) = \tau O(\mu(u t)) = \Oapp(\mu(u t))$}edge from parent[draw=none]}
            }
          }
        }
    }
  }
};
\path (q) -- (r) node [midway] {+};
\path (s) -- (r) node [midway] {+};
\path (s) -- (t) node [midway] {+};
\draw[<-,>=latex,dashed] (s) to (l);
\path (t) -- (u) node [midway] {+};
\draw[->,>=latex,dashed] (z) to (u);
\draw[->,>=latex,dashed] (j) to (t);
\draw[->,>=latex,dashed] (p) to (q);
\path (q) -- (v) node [midway] {$\displaystyle =$};
\end{tikzpicture}
\end{turn}}
\caption{Modular Reduction From Product Tree}\label{fig:div-prod-tree}
\end{figure}

\begin{algorithm}
\newcommand{\vstart}{\ensuremath{\mathrm{start}}}
\newcommand{\vmid}{\ensuremath{\mathrm{mid}}}
\newcommand{\vend}{\ensuremath{\mathrm{end}}}
\begin{algorithmic}[1]
\Require{$a\in\mathbb{N}$, $\pi$ the product tree of Algorithm \ref{alg:prod-tree}}
\Ensure{$A[i] = a \bmod \pi_i$ for $i \in \{1,\dots,2t-1\}$, computed as in Figure \ref{alg:div-prod-tree}}
\State $A \gets $ array of size $t$
\Function{modTree}{$i$}
  \If{$i < 2t$}
    \State $A[i] \gets A[\lfloor i/2 \rfloor] \bmod \pi_i$
    \State \Call{modTree}{$2i$}
    \State \Call{modTree}{$2i+1$}
  \EndIf
\EndFunction
\State $A[1] \gets a \bmod \pi_1$
\State \Call{modTree}{$2$}
\State \Call{modTree}{$3$}
\end{algorithmic}
\caption{Division Using a Product Tree}\label{alg:div-prod-tree}
\end{algorithm}

\subsection{Adapting $p$}

Let $\mbox{{\tt Prime}}[i]$ denote the $i$-th prime\footnote{with $\mbox{{\tt Prime}}[1]=2$}. Besides conditions on size, the {\sl only} property required from $p$ is to be co-prime with all the $h_i$ and all the $h'_i$. We can hence consider the following variants:\comm{What is the goal of the following variants? Is it to reduce the complexity of the generation of the primes?}

\subsubsection{Variant 1:} Smooth $p_i$:

$$p_i=\prod_{j=r_i}^{r_{i+1}-1} \mbox{{\tt Prime}}[j]$$

Where the bounds $r_i$ are chosen to ensure that each $p_i$ has the proper size.

\subsubsection{Variant 2:} $p_i=\mbox{{\tt Prime}}[i]^{r_i}$ where the exponents $r_i$ are chosen to ensure that each $p_i$ has the proper size.

\subsubsection{Variant 3:} Progressively work modulo a growing power of two. This variant, is probably the most efficient of all, but somewhat complex to explain. We hence describe it in detail in Appendix \ref{powtwo}. This variant is compatible with the use of {\sc fft}-multiplication, hence asymptotic complexity is preserved. In addition, it avoids all modular reductions and all {\sc crt} re-combinations and hence offers considerable constant-factor accelerations.\smallskip

\begin{verbatim}

Have this written by Fabrice...

(les modulos etant $2^{ut}, 2^{2ut}, ...$):
- calcul des $H_j$ comme dans 4.1 par product tree
- produit des $H_j$ de la facon suivante pour round 1:
  - $\mbox{prod} = 1$ (de taille $ut$)
  - pour $j=1,...$
      $\mbox{carry}_j | \mbox{prod} = \mbox{prod} \times H_j$
  - retourner prod, et garder carry
- produit des $H_j$ pour round suivants:
  - prod = 0
  - pour $j=1,...$
      $\mbox{carry}_j | \mbox{prod} = \mbox{prod} \times H_j + \mbox{carry}_j$

L'idee est la suivante (invariant): au round i et au tour j, prod contient les bits $(i-1)ut,...iut$ du produit reel de $H_1,...,H_j$ et
$\mbox{carry}_j$ contient le carry genere par ces bits...
\end{verbatim}

\section{Implementation}
\label{program}

We implemented and benchmarked the Divide \& Factor protocol described in the previous sections. The implementation is called \btrsync, its source code is available from~\cite{Robin}.\smallskip

\comm{on parle de "in the previous sections" mais de quelle variate s'agit il?}

The program performs unidirectional synchronization, which is simpler to understand. The code is divided into two subprograms: a shell script and a Python program:

\subsubsection{The Shell Script} sets up two instances of the Python program on Oscar and Neil and establishes a bidirectional communication channel between them using two Unix pipes between their standard inputs and outputs.

\subsubsection{The Python Program} uses {\sf gmp} to perform all the number theory operations and performs the actual synchronization. It proceeds in two phases:

\subsubsection{Finding Different Files}

\begin{enumerate}
\item Hash of files' contents concatenated with their paths, types (folder/file), and permissions (not supported yet).
\item Implement the protocol proposed in Section \ref{???} with input data coming from {\tt stdin} and output data going to {\tt stdout}.
\end{enumerate}
\comm{encore une fois "Implement the protocol proposed in Section \ref{???}" qu'a t on implemente au juste?}
More precisely:
\begin{itemize}
\item Oscar sends it product of hashes modulo a first prime number $p_1$.
\comm{sur "a first prime number $p_1$", l'implem marche avec $p_i$ ou des $2^u$?}
\item Neil receives the product, divides by its own product of hashes, reconstructs the fraction modulo $p_1$ and checks if he can factor the denominator using his hashes base. If he can, he stops and sends the numerator and the list of tuples (path, type, hash of content of the file) corresponding to the denominator's factors. Otherwise he sends "None" [is this the ASCII string "None"? if not what does he send precisely?].
\item If Neil sent "None", Oscar computes the product of hashes modulo another prime $p_2$, sends it... CRT mechanism... [can we elaborate more on what happens here? which functions in GMP are used to do the CRT?]
\item If Neil sent the numerator and a list of tuples, then Oscar factors the numerator over his own hash values. Now each party (Neil, Oscar) knows precisely the list of files (path + type + hash of content) that differs from the other party.
\end{itemize}

[please structure the following:]\smallskip

2. synchronize all the files. This part is not completely optimized.\smallskip

We just remove all folders Oscar should not have and create new folders.\smallskip

Then we remove all files Oscar should not have and synchronize using rsync the last files.\smallskip

We could check for move (since we have the list of hash of contents of files) and do moves locally.\smallskip

We can even try to detect moves of complete subtrees...\smallskip

\subsection{Move Resolution Algorithm}

To reproduce the structure of Oscar on Neil's disk, we need to perform a sequence of file moves. Sadly, it is not straightforward to apply the moves, because, if we take a file to move, its destination might be blocked, either because a file already exists (we want to move $a$ to $b$, but $b$ already exists), or because a folder cannot be created (we want to move $a$ to $b/c$, but $b$ already exists as a file and not as a folder). Note that for a move operation $a \rightarrow b$, there is at most one file blocking the location $b$: we will call it the {\sl blocker}.

If the blocker is absent on Oscar, then we can just delete the blocker. However, if a blocker exists, then we might need to move it somewhere else before we solve the move we are interested in. This move itself might have a blocker, and so on. It seems that we just need to continue until we reach a move which has no blocker or whose blocker can be deleted, but we can get caught in a cycle: if we must move $a$ to $b$, $b$ to $c$ and $c$ to $a$, then we will not be able to perform the operations without using a temporary location.

How can we perform the moves? A simple way would be to move each file to a unique temporary location and then rearrange files to our liking: however, this performs many unnecessary moves and could lead to problems if the program is interrupted. We can do something more clever by performing a decomposition in Strongly Connected Components ({\sc scc}) of the {\sl move graph} (with one vertex per file and one edge per move operation going from to the file to its blocker or to its destination if no blocker exists). The computation of the {\sc scc} decomposition is simplified by the observation that because two files being moved to the same destination must be equal, we can only keep one arbitrary in-edge per node, and look at the graph pruned in this fashion: its nodes have in-degree at most one, so the strongly connected components are either single nodes or cycles. Once the {\sc scc} decomposition is known, the moves can be applied by applying each {\sc scc} in a bottom-up fashion, an {\sc scc}'s moves being solved either trivially (for single files) or using one intermediate location (for cycles).

The detailed algorithm is implemented as two mutually recursive functions and presented as Algorithm \ref{alg:moves}.

% TODO check this algo
\begin{algorithm}
  \caption{Perform Moves}
  \label{alg:moves}
  \begin{algorithmic}[1]
    \Require{$\mathfrak{D}$ is a dictionary where $\mathfrak{D}[f]$ denotes the intended destinations of $f$}
    \Statex
    \Let{$M$}{[]}
    \Let{$T$}{[]}
    \For{$f$ in $\mathfrak{D}$'s keys}
      \Let{$M[f]$}{not\_done}
    \EndFor
    \Function{unblock\_copy}{$f, t$}
      \If{$t$ is blocked by some $b$}
        \If{$b$ is not in $\mathfrak{D}$'s keys}
          \State unlink($b$) \Comment{We don't need $b$}
        \Else
          \State \Call{resolve}{$b$} \Comment{Take care of $b$ and make it go away}
        \EndIf
      \EndIf
      \If{$T[f]$ was set}
        \Let{$f$}{$T[f]$}
      \EndIf
      \State copy($f$, $d$)
    \EndFunction
    \Function{resolve}{$f$}
      \If{$M[f] =$ done}
        \State \Return \Comment{Already managed by another in-edge}
      \EndIf
      \If{$M[f] =$ doing}
        \Let{$T[f]$}{mktemp()}
        \State move($f$, $T[f]$)
        \Let{$M[f]$}{done}
        \State \Return \Comment{We found a loop, moved $f$ out of the way}
      \EndIf
      \Let{$M[f]$}{doing}
      \For{$d \in \mathfrak{D}[f]$}
        \If{$d \neq f$}
          \State unblock\_copy($f$, $d$) \Comment{Perform all the moves}
        \EndIf
      \EndFor
      \If{$f \notin \mathfrak{D}[f]$ and $T[f]$ was not set}
        \State unlink($f$)
      \EndIf
      \If{$T[f]$ was set}
        \State unlink($T[f]$)
      \EndIf
    \EndFunction

    \For{$f$ in $\mathfrak{D}$'s keys}
      \State \Call{resolve}{$f$}
    \EndFor
  \end{algorithmic}
\end{algorithm}

An optimization implemented by \btrsync over the algorithm described here is to move files instead of copying them and then remove the original file. Moves are faster than copies on most filesystems as the OS does not need to copy the actual file contents to perform moves.

\subsection{Experimental Comparison to \rsync}

We compared \rsync\footnote{\rsync version 3.0.9, used both as a competitor to benchmark against and as an underlying call in our own code.}
and our Divide \& Factor implementation (called \btrsync) under the following experimental conditions:

\subsubsection{Test Directories:} The directories used for transmission and time
comparisons are described in Table~\ref{tab:benchdirec}.\smallskip

\subsubsection{Command-Line Options:} \rsync was called with the following options, for the reasons below:

$\blacktriangleright$ {\tt --delete} to delete existing files on Oscar which do
not exist on Neil like \btrsync does.\smallskip

$\blacktriangleright$ {\tt -I} to ensure that \rsync did not cheat by looking at file modification times (which \btrsync does not do).\smallskip

$\blacktriangleright$ {\tt --chmod="a=rx,u+w"} in an attempt to disable the transfer of file permissions (which \btrsync does not transfer). Although these settings ensure that \rsync does not need to transfer permissions, verbose logging suggests that it does transfer them anyway, so \rsync must lose a few bytes per file as compared to \btrsync for this reason.\smallskip

$\blacktriangleright$ {\tt -v} Transmission accounting was performed by calling \rsync with the {\tt -v} flag (which reports the number of sent and received bytes). For \btrsync we added a piece of code counting the amount of data transmitted during \btrsync's own negotiations.\smallskip

\subsubsection{Network Configuration:} Experiments were performed without any network transfer, by synchronizing two folders on the same host. Hence, time measurements should mostly represent the {\sc cpu} cost of the synchronization.

\subsubsection{Results:}

Results are given in Table \ref{tab:results}. In general, \btrsync spent more time than \rsync on computation (especially when the number of files is large, which is typically seen in the experiments involving {\tt synthetic}). Transmission results, however, turn out to be favorable to \btrsync.\smallskip

In the trivial experiments where either Oscar or Neil have no data at all, \rsync outperforms \btrsync. This is especially visible when Neil has no data: \rsync immediately notices that there is nothing to transfer, but \btrsync engages in information transfers to determine the symmetric
difference.\smallskip

On non-trivial tasks, however, \btrsync outperforms \rsync. This is the case of the {\tt synthetic} datasets, where \btrsync does not have to transfer
information about all unmodified files, and even more so in the case where there are no modifications at all. For Firefox source code datasets, \btrsync saves a very small amount of bandwidth, presumably because of unmodified files. For the \btrsync source code dataset, we notice that \btrsync, unlike \rsync, was able to detect the move and avoid retransferring the moved folder.

\section{Conclusion and Further Improvements}

\comm{V√©rifier les claims de cette liste, et en parler d√®s l'intro.}
The main contributions of our work are:

\begin{itemize}
  \item We present the novel ``Divide \& Factor'' protocol for set reconciliation, which is based on number theory and is optimal with respect to transfer size.
  \item We study the problem of set reconciliation of directories of files. We
    discuss the optimal size of message digests in this setting, as well as a
    move resolution algorithm to reproduce a directory structure.
  \item We present \btrsync, an open source implementation of the ``Divide \& Factor'' protocol.
  \item We demonstrate the usability of this implementation through benchmarks on synthetic and real-world tasks, and show that \btrsync exchanges less data than the popular software \rsync.
  \item The optimizations presented in this paper apply to~\cite{} as well.\comm{TODO r√©soudre ce point.}
\end{itemize}

Many fine questions of the probabilistic discussions in the paper are left as future work.\comm{Be more specific!} Another further line of research would be to pursue development of \btrsync to make it suitable for end users.

\section{Acknowledgment}

The authors acknowledge Guillain Potron for his early involvement in this research work.\smallskip

\section{ToDo}

\begin{itemize}

\item Pourquoi on a deux biblios?! Laquelle est la bonne ?

\item Merge two reference files rsynch and wagner.\smallskip

\item @Fabrice: Fig. 1 pas clair, je pense qu'il faut mieux cr√©er une notation mu gcd car en fait la FFT ne modifie que le mu... Et on pourrait plut\^ot indiquer dans ce tableau la complexit√© (avec mu et mu gcd) en utilisant l'optimisation d√©crite ou sans utiliser l'optimisation d√©crite). N'oublions pas de pr√©ciser que les temps des exp√©riences d'Antoine comptent aussi le tirage des nombres premiers (qui doit \^etre n√©gligeable peut-\^etre dans notre cas, je ne me rappelle plus...)

\item @Fabrice: Pour √©viter le cas empty $\rightarrow$ source trop gros, on pourrait imaginer l'astuce suivante: si jamais Neil la taille de c est plus petite que la taille du produit des nombres premiers $p_1$...$p_n$ utilis√©s, Neil envoie un message pour l'indiquer, et on arr\^ete l√† le protocole. Et Oscar peut directement factoriser ce nombre envoy√©...

\item @Fabrice: il faut discuter de la taille de la taille maximale des ``petits'' premiers utilis√©s pour les variantes de $p$ et montrer que cela n'enl√®ve pas trop d'entropie pour les $h_i$. Encore une fois, je m'en occupe la semaine prochaine si besoin.

\item Refaire une derni√®re fois les exp√©riences, vu que Fabrice a significativement am√©lior√© les perfs.

\item Faire clarifier par Fabrice l'histoire du doublement...

\item Prendre en compte les remq suivants de Fabrice
\begin{verbatim}

- je pense que l'√©v√©nement bottom_bandwidth est ou bien l'√©v√©nement "on n'a pas r√©ussi √† factoriser" ou bien "on n'a pas r√©ussi √† factoriser ou bien %la factorisation obtenue est abusive". Dans l'article, il semble que tu as choisi le premier, donc je pense que la phrase "bottom_... went undetected" n'est pas tout √† fait correcte, mais je me trompe peut-√™tre. J'aurai dis "even if bottom_... are not output, this does not mean the protocol is a success, since there may be misfactorization..."

- je pense que tu essayes de traiter 2 cas ensembles: modulus trop petit (t0 > t) et collisions. Le probl√®me est que ces deux probl√®mes sont compliqu√©s, et que j'ai peur que le lecteur soit embrouill√©... Et en fait, ton protocole fig. 2/3 ne traite que le probl√®me de collisions d'une certaine mani√®re, ... car tu ne change pas le "t" ni ne pr√©cise d'utiliser le CRT pour augmenter la taille du modulus.
- dans l'intro, au lieu de "This article proposes a new reconciliation protocol based on number theory. In terms
of asymptotic transmission complexity, the proposed procedure reaches optimality as well."
j'aurai dis quelque chose comme:
"In this article, we first propose a new reconciliation protocol based on number theory. In terms
of asymptotic transmission complexity, the proposed procedure reaches optimality as well.

Then, we show how to apply this set reconciliation protocol to efficiently synchronize real files.
We show that the naive method of hashing the files and synchronizing these hashes can be improved in two ways.
First, we show that the size of the hashes can be carefully optimize to reduce the bandwidth by a constant factor compared to a naive choice of his size (to avoid all collisions.
Then, we show that a protocol to detect moves of files and carefully use this information to avoid transmitting files which have only be moved.
Finally, we run some practical experiments and compare ... rsync ..."
\end{verbatim}
\end{itemize}

\nocite{rsync}
\nocite{wagner}

\bibliographystyle{splncs03}
\bibliography{btrsync}

\begin{thebibliography}{30}

\bibitem{boneh} D. Boneh, {\sl Finding Smooth Integers in Short Intervals Using CRT Decoding}, Proceedings of the 32-nd Annual ACM Symposium on Theory of Computing, 2000, pp. 265--272.

\bibitem{phong} D. Bleichenbacher and Ph. Nguyen, {\sl Noisy Polynomial Interpolation and Noisy Chinese Remaindering}, Advances in Cryptology -- Proceedings of Eurocrypt 2000, vol. 1807 of Lecture Notes in Computer Science, Springer-Verlag, pp. 53--69.

\bibitem{PSRec} Y. Minsky, A. Trachtenberg, {\sl Scalable Set Reconciliation}, 40th Annual Allerton Conference on Communications, Control and Computing, Monticello, {\sc il}, October 2002. A full version entitled {\sl Practical Set Reconciliation} can be downloaded from \url{http://ipsit.bu.edu/documents/BUTR2002-01.ps}

\bibitem{Mins1} Y. Minsky, A. Trachtenberg, R. Zippel, {\sl Set reconciliation with nearly optimal communication complexity}. {\sc ieee} Transactions on Information Theory, 49(9), 2003, pp. 2213‚Äì2218.

\bibitem{Whats} D. Eppstein, M. Goodrich, F. Uyeda, G. Varghese What's the difference?: efficient set reconciliation without prior context
ACM SIGCOMM Computer Communication Review - {\sc sigcomm}'11, 41(4), 2011, pp. 218-229.

\bibitem{Robin} \url{https://github.com/RobinMorisset/Btrsync}

\end{thebibliography}

\appendix

\section{Extended Protocol}
\label{sec:extended}

\begin{center}
\begin{tabular}{lcl}\toprule
\multicolumn{3}{c}{{\sf First phase during which Neil amasses modular
information on the difference~~}} \\\midrule
~~{\bf Oscar}                      &                                                      &   {\bf Neil}~\\
                                   &                                                      &start the protocol with $p_1$~\\
                                   &~~{{\LARGE $\stackrel{c_1}{\longrightarrow}$}}~~      &   \\
                                   &                                                      &computes $a,b$ using $p_1$~\\
                                   &                                                      &if $a$ factors properly then go to {\sf Final Phase}\\
                                   &                                                      &~~~~~~else perform the protocol with $p_2$~~\\
                                   &~~{{\LARGE $\stackrel{c_2}{\longrightarrow}$}}~~      &   \\
                                   &                                                      &computes $c \bmod p_1 p_2=\mbox{CRT}_{p_1,p_2}(c_1,c_2)$~~\\
                                   &                                                      &computes $a,b$ using $p_1 p_2$~\\
                                   &                                                      &if $a$ factors properly then go to {\sf Final Phase}\\
                                   &                                                      &~~~~~~else perform the protocol with $p_3$~~\\
                                   &~~{{\LARGE $\stackrel{c_3}{\longrightarrow}$}}~~      &   \\
                                   &                                                      &computes $c \bmod p_1 p_2 p_3=\mbox{CRT}_{p_1,p_2,p_3}(c_1,c_2,c_3)$~~\\
                                   &                                                      &computes $a,b$ using $p_1 p_2 p_3$~\\
                                   &                                                      &if $a$ factors properly then go to {\sf Final Phase}\\
                                   &                                                      &~~~~~~else perform the protocol with $p_4$ ~~\\
                                   &                  $\vdots$
& \\\midrule
\multicolumn{3}{c}{{\sf Final Phase~~}} \\\midrule
                                   &                                                      & \\
                                   &                                                      &Let $\mathfrak{S}=\{F'_i \mbox{~s.t.~} a \bmod h'_i =0\}$~~\\
                                   &~~{\LARGE $\stackrel{\mathfrak{S},b}{\longleftarrow}$}&\\
                                   ~~deletes files s.t. $b \bmod h_i =0$&                                                      &\\
                                   ~~adds $\mathfrak{S}$ to the disk
                                   &
                                   &\\\bottomrule
\end{tabular}
\end{center}

Note that parties do not need to store the $p_i$'s in full. Indeed, the $p_i$s could be subsequent primes sharing their most significant bits. This reduces storage per prime to a very small additive constant $ \cong \mbox{ln}(p_i) \cong \mbox{ln}(2^{2tu+2}) \cong 1.39(tu+1)$ of about $\log_2(tu)$ bits.

\section{Power of Two Protocol}
\label{powtwo}

In this variant Oscar computes $c$ in $\mathbb{N}$:

$$
c=\prod_{F_i\in \mathfrak{F}} \mbox{{\tt HashPrime}}(F_i) = \prod_{i=1}^n h_i \in \mathbb{N}
$$

and considers $c=\bar{c}_{n-1}|\ldots|\bar{c}_2|\bar{c}_0$ as the concatenation of $n$ successive $u$-bit strings. Again, we omit the treatment of $\bot$s for the sake of clarity.

\begin{center}
\begin{tabular}{|lcl|}\hline
\multicolumn{3}{|c|}{{\sf First phase during which Neil amasses modular information on the difference~~}} \\\hline
~~{\bf Oscar}                      &                                                      &   {\bf Neil}~\\
~~computes $c\in \mathbb{N}$       &                                                      &\\
                                   &~~{{\LARGE $\stackrel{\bar{c}_0}{\longrightarrow}$}}~~&   \\
                                   &                                                      &computes $a,b$ modulo $2^u$~\\
                                   &                                                      &if $a$ factors properly then go to {\sf Final Phase}~~\\
                                   &                                                      &~~~~~~else request next chunk $\bar{c}_1$~~\\
                                   &~~{{\LARGE $\stackrel{\bar{c}_1}{\longrightarrow}$}}~~&   \\
                                   &                                                      &construct $c \bmod 2^{2u}=\bar{c}_1|\bar{c}_0$~~\\
                                   &                                                      &computes $a,b$ modulo $2^{2u}$~\\
                                   &                                                      &if $a$ factors properly then go to {\sf Final Phase}~~\\
                                   &                                                      &~~~~~~else request next chunk $\bar{c}_2$~~\\
                                   &~~{{\LARGE $\stackrel{\bar{c}_2}{\longrightarrow}$}}~~      &   \\
                                   &                                                      &construct $c\bmod 2^{3u}=\bar{c}_2|\bar{c}_1|\bar{c}_0$~~\\
                                   &                                                      &computes $a,b$ modulo $2^{3u}$~\\
                                   &                                                      &if $a$ factors properly then go to {\sf Final Phase}~~\\
                                   &                                                      &~~~~~~else request next chunk $\bar{c}_3$~~\\
                                   &                  $\vdots~(\mbox{~for~}2t \mbox{~rounds~})$    & \\\hline\hline
\multicolumn{3}{|c|}{{\sf Final Phase~~}} \\\hline
                                   &                                                      & \\
                                   &                                                      &Let $\mathfrak{S}=\{F'_i \mbox{~s.t.~} a \bmod 2^{2tu} =0\}$~~\\
                                   &~~{\LARGE $\stackrel{\mathfrak{S},b}{\longleftarrow}$}&\\
                                   ~~deletes files s.t. $b \bmod 2^{2t u} =0$&        &\\
                                   ~~adds $\mathfrak{S}$ to the disk    &                                                      &\\\hline
\end{tabular}
\end{center}

\section{Hashing Into Primes}
\label{sec:hashprime}
Hashing into primes is frequently needed in cryptography. A recommended implementation of $\mbox{{\tt HashPrime}}(F)$ is given in Algorithm \ref{alg:primes}. If $u$ is large enough ({\sl e.g.} $160$) one might sacrifice uniformity to avoid repeated file hashings by defining $\mbox{{\tt HashPrime}}(F)=\mbox{{\tt NextPrime}}(\mbox{{\tt Hash}}(F))$. Yet another acceleration (that further destroys uniformity) consists in replacing $\mbox{{\tt NextPrime}}$ by Algorithm \ref{alg:scanprime} where $\alpha=2\times 3\times 5\times \cdots \times \mbox{{\tt Prime}}[d]$ is the product of the first primes until some rank $d$.

\begin{algorithm}
  \caption{Fast Nonuniform Hashing Into Primes}
  \label{alg:scanprime}
  \begin{algorithmic}[1]
  \State $h =\alpha \left\lfloor\frac{\mbox{{\tt Hash}}(F)}{\alpha}\right\rfloor+1$
\While{$h$ is composite}
\State $h = h-\alpha$
\EndWhile
\State \Return{$h$}
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}
  \caption{Possible Implementation of $\mbox{{\tt HashPrime}}(F)$}
  \label{alg:primes}
  \begin{algorithmic}[1]
  \State $i=0$
\Repeat
\State $h = 2\cdot\mbox{{\tt Hash}}(F|i)+1$
\State $i = i+1$
\Until{$h$ is prime}
\State \Return{$h$}
  \end{algorithmic}
\end{algorithm}

\begin{table}
  \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}ll r r r r r r r r }
    \toprule
    \multicolumn{2}{c}{\bf Entities and Datasets} & \multicolumn{6}{c}{\bf Transmission (Bytes)} & \multicolumn{2}{c}{\bf Time (s)} \\
    \midrule {\bf \hfill Neil's $\mathfrak{F}'$ \hfill \null} & {\bf \hfill Oscar's $\mathfrak{F}$ \hfill \null}
    & $\mbox{{\tt TX}}_{\mbox{{\tiny {\tt rs}}}}$ & $\mbox{{\tt RX}}_{\mbox{{\tiny {\tt rs}}}}$  & $\mbox{{\tt TX}}_{\mbox{{\tiny {\tt bt}}}}$  & $\mbox{{\tt RX}}_{\mbox{{\tiny {\tt bt}}}}$  & $\delta_{\mbox{{\tiny {\tt rs}}}}-\delta_{\mbox{{\tiny {\tt bt}}}}$ &
    $\frac{\delta_{\mbox{{\tiny {\tt bt}}}}}{\delta_{\mbox{{\tiny {\tt rs}}}}}$ & $\mbox{{\tt time}}_{\mbox{{\tiny {\tt rs}}}}$ & $\mbox{{\tt time}}_{\mbox{{\tiny {\tt bt}}}}$ \\\midrule
    &&&&&&&&&\\[-1em]
    \input{results.tex}
    \bottomrule
  \end{tabular*}

  \caption{Experimental results. {\tt rs} and {\tt bt} subscripts respectively denote {\tt rsync} and {\tt btrsync}. The two first columns indicate the datasets. Synchronization is performed {\sl from} Neil {\sl to} Oscar. {\tt RX} and {\tt TX} denote the quantity of received and sent bytes and $\delta_{\square}=\mbox{{\tt TX}}_{\square}+\mbox{{\tt RX}}_{\square}$. $\delta_{\mbox{{\tiny {\tt rs}}}}-\delta_{\mbox{{\tiny {\tt bt}}}}$ and ${\delta_{\mbox{{\tiny {\tt bt}}}}}/{\delta_{\mbox{{\tiny {\tt rs}}}}}$ express the absolute and the relative differences in transmission between the two programs. The last two columns show timing results.}
  \label{tab:results}
\end{table}
\comm{Antoine sur quelle plate-formes ont ete obtenus les resultats experimentaux? quelles vitesses de processeur etc?}

\begin{table}
\begin{center}
\begin{tabular}{ll}\toprule
~~{\bf Directory}              ~~&~~{\bf Description}\\\midrule
~~{\tt synthetic}              ~~&~~A directory containing 1000 very small files containing~~\\
~~                             ~~&~~the numbers $1,2,\ldots,1000$. \\
~~{\tt synthetic\_shuffled}    ~~&~~{\tt synthetic} with:\\
                             ~~& ~~~~~10 deleted files\\
                             ~~& ~~~~~10 renamed files \\
                             ~~& ~~~~~10 modified files \\
~~{\tt source}                 ~~& ~~A snapshot of \btrsync's own source tree \\
~~{\tt source\_moved}          ~~& ~~{\tt source} with one big folder (a few megabits) renamed.~~\\
~~{\tt firefox-13.0}           ~~& ~~The source archive of Mozilla Firefox 13.0.\\
~~{\tt firefox-13.0.1}         ~~& ~~The source archive of Mozilla Firefox 13.0.1\\
~~{\tt empty}                  ~~& ~~An empty folder.\\\bottomrule
\end{tabular}\smallskip
  \caption{Test Directories.}
  \label{tab:benchdirec}
\end{center}
\end{table}



\end{document}
