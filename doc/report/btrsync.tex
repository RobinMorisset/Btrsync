\documentclass[11pt]{llncs}

\def\makeitbig{%
\setlength{\textwidth}{15.9cm}%
\setlength{\oddsidemargin}{.01cm}%
\setlength{\evensidemargin}{.01cm}%
\setlength{\textheight}{21.5cm}%
\setlength{\topmargin}{-.25cm}%
\setlength{\headheight}{.7cm}%
\leftmargini 20pt     \leftmarginii 20pt%
\leftmarginiii 20pt   \leftmarginiv 20pt%
\leftmarginv 12pt     \leftmarginvi 12pt%
\pagestyle{myheadings}}%

\makeitbig

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsfonts, amssymb, graphicx, rotating, epsfig}
\usepackage{verbatim}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{url}
\usepackage{tikz}
\usepackage{tabularx,multirow}
\usepackage{xspace}

\newcommand{\setup}{$\mbox{{\sf Generate}}$ }
\newcommand{\sign}{$\mbox{{\sf Sign}}$ }
\newcommand{\ver}{$\mbox{{\sf Verify}}$ }
\newcommand{\sys}{$\mbox{{\sf System parameters}}$ }

\newcommand{\sig}{{\sf $($Gene\-rate, Sign, Verify$)$} }
\newcommand{\ignore}[1]{}
\newcommand{\btr}{{\tt btrsync}}
\newcommand{\rsy}{{\tt rsync}}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\Cov}[0]{\mbox{Cov}}
\newcommand{\Var}[0]{\mbox{Var}}
\newcommand{\xor}[0]{\oplus}
\newcommand{\rmu}[0]{\mbox{RM}}
\newcommand{\Prob}[1]{{\Pr\left[\,{#1}\,\right]}}
\newcommand{\EE}[1]{{\mathbb{E}\left[{#1}\right]}}
\newcommand{\Oapp}{\ensuremath{\tilde{O}}}

\newcommand{\btrsync}{\texttt{btrsync}\xspace}
\newcommand{\rsync}{\texttt{rsync}\xspace}

\usepackage{hyperref}

\begin{document}

\title{When File Synchronization Meets Number Theory}

\author{Antoine Amarilli \and Fabrice Ben Hamouda \and Florian Bourse \and\\
Robin Morisset \and David Naccache \and Pablo Rauzy}

\institute{
\'{E}cole normale sup\'{e}rieure, D\'{e}partement d'informatique \\
   45, rue d'Ulm, {\sc f}-75230, Paris Cedex 05, France.\\
   \email{surname.name@ens.fr} (except for \email{fabrice.ben.hamouda@ens.fr})
}

\maketitle

\begin{abstract}

In this work we [to be completed by David]

\end{abstract}

\section{Introduction}

This work revisits {\sl set reconciliation}, a problem consisting in synchronizing two multisets whiles minimizing communication complexity. Set reconciliation has many practical applications, the most typical of which is certainly the incremental backup of information.\smallskip.

Set reconciliation has already efficient and elegant solutions. For instance, \cite{PSRec} presents a reconciliation protocol whose computational and communication complexities are linear in the number of differences in the reconciled multisets.\smallskip

We refer the reader to \cite{PSRec,Mins1,Whats} (to quote only a few typical references) for more on the problem's history and the existing solutions.\smallskip

This article proposes a new reconciliation protocol based on number theory. In terms of communication complexity, the proposed procedure is comparable to prior publications \cite{PSRec} (that anyhow reached optimality) but its main interest lies in the fact that the correctness of the proposed protocol can be reduced to the cryptographic security of specific signature padding schemes. This proof stems from the application of the approach described in \cite{comparing}.\smallskip

In addition, the new protocol offers new parameter trade-offs and hence the possibility that specific implementations would offer {\sl constant}-factor gains over alternative asymptotically-equivalent solutions.\smallskip

\section{A New Set Reconciliation Protocol}

\subsection{A Few Notations}

We model the directory synchronization problem as follows: Oscar possesses an old version of a directory $\mathfrak{D}$ that he wishes to update. Neil has the up-to-date version $\mathfrak{D}'$. The challenge faced by Oscar and Neil\footnote{Oscar and Neil will respectively stand for {\sl \underline{o}ld} and {\sl \underline{n}ew}.} is that of {\sl exchanging as little data as possible} during the synchronization process. In practice $\mathfrak{D}$ and $\mathfrak{D}'$ usually differ both in their files and in their tree structure.\smallskip

In tackling this problem this paper separates the {\sl ``what''} from the {\sl ``where''}: namely, we disregard the relative position of files in subdirectories and model directories as multisets of files. Let $\mathfrak{F}$ and $\mathfrak{F}'$ denote the multisets of files contained in $\mathfrak{D}$ and $\mathfrak{D}'$. We denote $\mathfrak{F}=\{F_0,\ldots,F_{n}\}$ and $\mathfrak{F}'=\{F'_0,\ldots,F'_{n'}\}$.\smallskip

Let $\mbox{{\tt Hash}}$ denote a collision-resistant hash function\footnote{{\sl e.g.} SHA-1} and let $F$ be a file. Let $\mbox{{\tt NextPrime}}(F)$ be the prime immediately larger than $\mbox{{\tt Hash}}(F)$ and let $u$ denote the size of {\tt NextPrime}'s output in bits. Define the shorthand notations: $h_i=\mbox{{\tt NextPrime}}(F_i)$ and $h'_i=\mbox{{\tt NextPrime}}(F'_i)$.\smallskip

TODO(amarilli): use the uniform nextprime (discussion of relative costs with respect to (1.) hashing costs and (2.) finding the next prime costs)

\subsection{Description of the Basic Exchanges}
\label{basic}

Let $t$ be the number of discrepancies between $\mathfrak{F}$ and $\mathfrak{F}'$ that we wish to spot, {\sl i.e.}:

$$t=\#\mathfrak{F}+\#\mathfrak{F}'-2 \#(\mathfrak{F} \bigcap \mathfrak{F}')$$

We generate a prime $p$ such that:

\begin{equation}
\label{equp}
2^{2ut+1} \leq p < 2^{2ut+2}
\end{equation}

Given $\mathfrak{F}$, Neil generates and sends to Oscar the redundancy:

$$
c=\prod_{i=1}^n h_i \bmod p
$$

Oscar computes:\smallskip

$$c'=\prod_{i=1}^n h'_i \bmod p{~~~\mbox{and}~~~}s=\frac{c'}{c} \bmod p$$

Using \cite{vallee} the integer $s$ can be written as:
$$s=\frac{a}{b} \bmod p{~~~\mbox{where the~}G_i\mbox{~denote files and~}}
\left\{
\begin{array}{lcr}
a & =&  \prod\limits_{G_i \in \mathfrak{F}'\wedge G_i \not\in\mathfrak{F}} \mbox{{\tt NextPrime}}(G_i) \\
\\
b & = & \prod\limits_{G_i \not\in\mathfrak{F}' \wedge G_i \in\mathfrak{F}} \mbox{{\tt NextPrime}}(G_i)
\end{array}
\right.
$$

Note that since $\mathfrak{F}$ and $\mathfrak{F}'$ differ by at most $t$ elements, $a$ and $b$ are strictly lesser than $2^{ut}$. Theorem \ref{theo} (see \cite{cryptorational}) guarantees $a$ and $b$ can be efficiently recovered from $s$ (this problem is known as {\sl Rational Number Reconstruction} \cite{pan2004rational,wang2003acceleration}). This is done using Gauss' algorithm for finding the shortest vector in a bi-dimensional lattice \cite{vallee}.

\begin{theorem}
\label{theo}
Let $a,b \in {\mathbb Z}$ such that $-A \leq a \leq A$ and $0<b \leq B$. Let $p>2AB$ be a prime and $s=a b^{-1} \mod p$.
Then $a,b$ can be recovered from $A,B,s,p$ in polynomial time.
\end{theorem}

Taking $A=B=2^{ut}-1$, (\ref{equp}) implies that $2AB<p$. Moreover, $0 \leq a \leq A$ and $0 <b \leq B$. Thus Oscar can
recover $a$ and $b$ from $s$ in polynomial time. By testing the divisibility of $a$ and $b$ by the $h_i$ and the $h'_i$, Neil and Oscar can
precisely and deterministically identify the discrepancies between $\mathfrak{F}$ and $\mathfrak{F}'$ and settle them.\smallskip

Formally, this is done as follows:\smallskip

\begin{center}
\begin{tabular}{|lcl|}\hline
~~{\bf Oscar}                       &                                                      &   {\bf Neil}~\\
                                   &~~{{\LARGE $\stackrel{c}{\longrightarrow}$}}~~        &   \\
                                   &                                                      &computes $a,b$~\\
                                   &                                                      &if $a$ doesn't factor into a product of $h'_i$s~\\
                                   &                                                      &~~~~~~then output $\bot$ and halt~\\
                                   &                                                      &~~~~~~else let $\mathfrak{S}=\{F'_i \mbox{~s.t.~} a \bmod h'_i =0\}$~~\\
                                   &~~{\LARGE $\stackrel{\mathfrak{S},b}{\longleftarrow}$}&\\
~~deletes files s.t. $b \bmod h_i =0$&                                                      &\\
~~adds $\mathfrak{S}$ to the disk    &                                                      &\\\hline
\end{tabular}
\end{center}

As we have just seen, the ``output $\bot$ and halt'' protocol breakout should actually never occur as long as bounds on parameter sizes are respected. However, a file synchronization procedure that works {\sl only} for a limited number of differences is not very useful in practice. Subsection \label{insuf} explains how to use the protocol even when differences exceed $t$, the informational capacity of the modulus $p$.

\subsection{Reducing Correctness to... The Security of RSA}
\label{rsa}

{\sc rsa} \cite{RSA} is certainly the most famous public-key cryptosystem. Recall that to generate \cite{RSA} system parameters, one defines a security parameter $k \in\mathbb{N}$ and an encoding function $\mu : \{0,1\}^* \rightarrow  \{0,1\}^k$, selects two distinct $k/2$-bit primes $p,q$ and computes $N=pq$.\smallskip

The public verification exponent is an integer $e \in {\mathbb Z}^{*}_{\phi(N)}$ and the corresponding secret signature exponent is $d=e^{-1} \bmod \phi(N)$.\smallskip

To sign a message m, the signer $\mathcal{S}$ returns the signature $y=\mu(m)^d \bmod{N}$.\smallskip

To check the signature $y$, the verifier tests if $y^e \bmod{N}\stackrel{?}{=}\mu(m)$.\smallskip

To reduce the correctness of the reconciliation protocol of section \ref{basic} to the security of {\sc rsa}, we add two further restrictions to the {\sc rsa} parameters: We additionally require that $e$ is prime and $e>n+n'$. We call this {\sc rsa} variant, {\sl prime-exponent} {\sc rsa}.\smallskip

We also identify (redefine) two reconciliation protocol parameters with the following {\sl prime-exponent}{\sc rsa} parameters:

\begin{itemize}
\item Replace $h_i=\mbox{{\tt NextPrime}}(F_i)$ by $h_i=\mu(F_i)$.\smallskip

\item Replace the modulus $p$ used in section \ref{basic} by the {\sc rsa} modulus $N$, {\sl i.e.} redefine:

$$c=\prod_{i=1}^{n}\mu(F_i)\bmod{N}\mbox{~~and~~}c'=\prod_{i=1}^{n'}\mu(F'_i)\bmod{N}$$
\end{itemize}

If there exist an algorithm $\cal A$ capable of computing a pair of multisets $(\mathfrak{F},\mathfrak{F}')$ on which the reconciliation protocol is ineffective\footnote{Our definition of the term {\sl ineffective} is: The protocol terminates considering that Oscar and Neil are in synchrony whilst they are not.}, this implies that at the end of the protocol Oscar gets a multiset $\mathfrak{R}\neq\mathfrak{F'}$ for which:

$$s=\frac{c}{c'}=1 \bmod N \Rightarrow \prod_{R_i\in \mathfrak{R}}\mu(R_i)=\prod_{F_i\in \mathfrak{F}'}\mu(F'_i)\bmod{N}$$

This is {\sl exactly} the setting presented in \cite{comparing}, where multiset equality is ascertained by testing that:

$$\prod_{i=1}^{n}\mu(F_i)\stackrel{?}{=}\prod_{i=1}^{n'}\mu(F'_i)\bmod{N}$$

Hence, the security proof given in \cite{comparing} implies the following theorem:

\begin{theorem}\label{b}
If there exists an algorithm $\cal A$ such that:
\begin{itemize}
\item $\cal A$ is capable of computing a pair of multisets $(\mathfrak{F},\mathfrak{F}')$ on which the reconciliation protocol is ineffective,
\item $\cal A$ runs in time $t_2(k,\eta)$ and succeeds in finding $(\mathfrak{F},\mathfrak{F}')$ with probability $\varepsilon_2(k,\eta)$, where $\eta=\max(\#\mathfrak{F},\#\mathfrak{F}')$.
\end{itemize}
then there exists a forger $\cal F$ that finds a prime-exponent {\sc rsa} forgery after $q_1(k,\eta)<2\eta$ queries to $\cal S$ and $t_1(k,\eta)=t_2(k,\eta)+{\cal O}(\eta^2)+ {\cal O}(\eta k^2)$ computing time, with probability $\varepsilon_1(k,\eta)=\varepsilon_2(k,\eta)$.\smallskip
\end{theorem}

\subsection{The Case of Insufficient Information}
\label{insuf}
To extend the protocol to an arbitrary $t$, Oscar and Neil agree on an infinite set of primes $p_1,p_2,\ldots$ As long as the protocol fails, Neil will keep accumulating information about the difference between $\mathfrak{F}$ and $\mathfrak{F}'$ as shown in appendix A. Note that no information is lost and that the distilled modular knowledge about the difference adds-up until it reaches a threshold sufficient to reconciliate $\mathfrak{F}$ and $\mathfrak{F}'$.\smallskip

To determine after each new $p_i$ round if the synchronization is over, as the interaction starts Neil will send to Oscar $\mbox{{\tt Hash}}(\mathfrak{F}')$. As long as Oscar's state does not match this target digest $\mbox{{\tt Hash}}(\mathfrak{F}')$, Oscar will continue the interaction.

\section{Efficiency Considerations}

In this section we explore two strategies to reduce the size of $p$ and hence improve transmission by {\sl constant factors} (from an asymptotic communication complexity standpoint, nothing can be done as the protocol already transmits information proportional in size to the difference to settle).

\subsection{Probabilistic Decoding: Reducing $p$}

Generate a prime $p$ about twice smaller than previously, namely:
\begin{equation}
\label{eqnewp}
2^{ut+w-1}<p \leq 2^{ut+w}
\end{equation}

where $w \geq 1$ is some small integer (say $w=50$). For large $\eta=\max(n,n')$ and $t$, the size of the new prime $p$ used in this section will be approximately half the size of the prime $p$ in section \ref{basic}. The new redundancy $c$ is calculated as previously and is hence also approximately twice smaller. As previously, we have:

$$
s=\frac{a}{b} \bmod p{~~~\mbox{and~}}
\left\{
\begin{array}{lcr}
a & =&  \prod\limits_{G_i \in \mathfrak{F}'\wedge G_i \not\in\mathfrak{F}} \mbox{{\tt NextPrime}}(G_i) \\
\\
b & = & \prod\limits_{G_i \not\in\mathfrak{F}' \wedge G_i \in\mathfrak{F}} \mbox{{\tt NextPrime}}(G_i)
\end{array}
\right.
$$

and since there are at most $t$ differences, we must have:
\begin{equation}
\label{eqab}
a b \leq 2^{ut}
\end{equation}

The difference with respect to section \ref{basic} is that we do not have a fixed bound for $a$ and $b$ anymore; equation (\ref{eqab}) only provides a bound for the {\sl product} $a b$. Therefore, we define a finite sequence of couples of integers:

$$(A_i,B_i)=(2^{wi},\lfloor \frac{p-1}{2 A_i} \rfloor)\mbox{~~where~~}B_i>1$$

For all $i>0$ we have $2 A_i B_i<p$. Moreover, from equations (\ref{eqnewp}) and (\ref{eqab}) there must exist at least one index $i$ such that $0 \leq a \leq A_i$ and $0 <b \leq B_i$. Then using Theorem \ref{theo}, given $(A_i,B_i,p,s)$ one can recover $a$ and $b$, and thus the difference between $\mathfrak{F}$ and $\mathfrak{F}'$.\smallskip

The problem is that (by opposition to the protocol of section \ref{basic}) we have no guarantee that such an $(a,b)$ is unique. Namely, we could (in theory) stumble by sheer bad luck over another $(a',b')$ satisfying (\ref{eqab}) for some index $i' \neq i$. We expect this to happen with negligible probability for large enough $w$, but this makes the modified protocol heuristic only.\smallskip

To ascertain correctness upon termination, the parties can can use the $\mbox{{\tt Hash}}(\mathfrak{F}')$ protocol preamble mentioned in section \ref{insuf}.

\subsection{The File Laundry: Reducing $u$}

What happens if we shorten $u$ in the basic protocol?\smallskip

As foreseen by the birthday paradox, we should start seeing collisions. Let us analyze the statistics governing the appearance of collisions.

Consider $\mbox{{\tt Hash}}$ as a random function from $\{0,1\}^*$ to $\{0,\dots,2^u-1\}$.
Let $X^1_i$ be the random variable equal to $1$ when the file $F_i$ collides with another file, and equal to $0$ otherwise.
Clearly, we have $\Prob{X_i = 1} \le \frac{\eta -1}{2^u}$.
The average number of colliding files is hence:
\[ \EE{\sum_{i=0}^{\eta-1} X_i} \le \sum_{i=0}^{\eta-1} \frac{\eta -1}{2^u} = \frac{\eta (\eta - 1)}{2^u}. \]
For instance, for $\eta=10^6$ files and 32-bit digests, the expected number of colliding files is less than $233$.\smallskip

That being said, note that a collision can only yield a {\sl false positive} and never a {\sl false negative}. In other words, while a collision may obliviate a difference\footnote{{\sl e.g.} make the parties blind to the difference between {\tt index.htm} and {\tt iexplore.exe}.} a collision can never create a nonexistent difference {\sl ex nihilo}.\smallskip

Hence, it suffices to replace $\mbox{{\tt Hash}}(F)$ by a chopped $\hbar_{k,u}(F)=\mbox{{\tt MAC}}_k(F) \bmod 2^u$ to quickly filter-out file differences by repeating the protocol for $k=1,2,\ldots$ At each iteration the parties will detect new files and new deletions, fix these and ``launder'' again the remaining multisets.\smallskip

Indeed, the probability that a stubborn file persists colliding decreases exponentially with the number of iterations $k$, if MACs are random and independent from each other. Assume that $\eta$ remains invariant between iterations. Define the following random variables:\smallskip

$$
\begin{array}{rcl}
X^{\ell}_i & = &
\left\{
\begin{array}{lcl}
1 & ~~~~&  \mbox{if file $F_i$ collides with another file during iteration $\ell$.}\\
\\
0 & ~~~~&  \mbox{otherwise.}
\end{array}
\right.\\
\\
Y_i = \prod_{\ell=1}^k X^{\ell}_i & = &
\left\{
\begin{array}{lcl}
1 & ~~~~&  \mbox{if file $F_i$ collides with another file for all the $k$ iterations.}\\
\\
0 & ~~~~&  \mbox{otherwise.}
\end{array}
\right.
\end{array}$$

By independence, we have:

 \[ \Prob{Y_i = 1} = \prod_{\ell=1}^k \Prob{X^{\ell}_i = 1} = \Prob{X^1_i = 1} \dots \Prob{X^k_i = 1} \le \left( \frac{\eta -1}{2^u} \right)^k. \]
Therefore the average number of colliding files is:
\[
 \EE{\sum_{i=0}^{\eta-1} Y_i} \le \sum_{i=0}^{\eta-1} \left( \frac{\eta -1}{2^u} \right)^k =  \eta \left(\frac{\eta - 1}{2^u}\right)^k.
\]

And the probability that at least one false positive will survive $k$ rounds is:

\[
\epsilon_k \le \eta \left(\frac{\eta - 1}{2^u}\right)^k
\]

For the previously considered instance\footnote{$\eta=10^6$,$u=32$.} we get $\epsilon_2 \le 5.43\%$ and $\epsilon_3 \le 2 \cdot 10^{-3}\%$.

\subsubsection{A more refined (but somewhat technical) analysis}

As mentioned previously, the parties can remove the files detected as different during the first iteration and only with common and colliding files. Now, the only collisions that can fool round $k$, are the collisions of a file-pairs $(F_i,F_j)$ such that $F_i$ and $F_j$ have both already collided during {\sl all the previous iterations}\footnote{Note that the observation does not mandate that $F_i$ and $F_j$ who necessarily collided {\sl which each other}. Indeed, we may witness during the first round $\hbar_{1,u}(F_1)=\hbar_{1,u}(F_2)$,$\hbar_{1,u}(F_3)=\hbar_{1,u}(F_4)$ while during the second round $\hbar_{2,u}(F_1)=\hbar_{2,u}(F_4)$,$\hbar_{2,u}(F_2)=\hbar_{2,u}(F_3)$.}. We call such collisions ``stutters''. Define the random variable:

$$
Z^\ell_i = 
\left\{
\begin{array}{lcl}
1 & ~~~~&  \mbox{if file $F_i$ stuttered during the $\ell$ first protocol iterations.}\\
\\
0 & ~~~~&  \mbox{otherwise.}
\end{array}
\right.
$$

Suppose that $\eta > 1$.
Set $Z^0_i = 1$ and write $p_\ell = \Prob{Z^{\ell-1}_{i} = 1 \text{ and } Z^{\ell-1}_{j} = 1} $ for all $\ell$ and $i \neq j$.
For $k \ge 1$, we have
\begin{align*}
\Prob{Z^k_i=1} &= \Prob{\exists j\neq i \text{, } X^k_{i,j} = 1 \text{, } Z^{k-1}_{i} = 1  \text{ and } Z^{\ell-1}_{j} = 1}  \\
&\le \sum_{j=0, j\neq i}^{\eta-1} \Prob{X^{k-1}_{i,j} = 1} \Prob{Z^{k-1}_{i} = 1 \text{ and } Z^{k-1}_{j} = 1}  \\
&\le \frac{\eta-1}{2^u} p_{k-1}
\end{align*}
Furthermore $p_0 = 1$ and
\begin{align*}
p_\ell &= \Prob{X^{\ell}_0 = X^{\ell}_1 \text{, } Z^{\ell}_{0} = 1 \text{ and } Z^{\ell}_{1} = 1}
  + \Prob{X^{\ell}_0 \neq X^{\ell}_1 \text{, } Z^{\ell}_{0} = 1 \text{ and } Z^{\ell}_{1} = 1} \\
&\le \Prob{X^{\ell}_0 = X^{\ell}_1 \text{, } Z^{\ell-1}_{0} = 1 \text{ and } Z^{\ell-1}_{1} = 1} \\
  &\quad+ \sum_{i \ge 2, j \ge 2} \Prob{X^\ell_{0,i} = 1 \text{, } X^\ell_{1,j} = 1 \text{, } Z^{\ell-1}_{0} = 1 \text{ and } Z^{\ell-1}_{1} = 1} \\
&= \Prob{X^{\ell}_0 = X^{\ell}_1} \Prob{Z^{\ell-1}_{0} = 1 \text{ and } Z^{\ell-1}_{1} = 1} \\
  &\quad+ \sum_{i \ge 2, j \ge 2} \Prob{X^\ell_{0,i} = 1} \Prob{X^\ell_{1,j} = 1} \Prob{Z^{\ell-1}_{0} = 1 \text{ and } Z^{\ell-1}_{1} = 1} \\
&\le \frac{1}{2^u} p_{\ell-1} + \frac{(\eta-2)^2}{2^{2u}} p_{\ell-1} = p_{\ell-1}\left(\frac{1}{2^u}  + \frac{(\eta-2)^2}{2^{2u}}\right)
\end{align*}
hence:
\[ p_\ell \le \left( \frac{1}{2^u} + \frac{(\eta-2)^2}{2^{2u}} \right)^\ell, \]
and
\[ \Prob{Z^\ell_i=1} \le \left( \frac{1}{2^u} + \frac{(\eta-2)^2}{2^{2u}} \right)^{k-1} \]
And finally, the survival probability of at least one false positive after $k$ iterations satisfies:
\[
\epsilon'_k \le \frac{\eta(\eta-1)}{2^u} \left( \frac{1}{2^u} + \frac{(\eta-2)^2}{2^{2u}} \right)^{k-1}
\]

For the $(\eta=10^6,u=32,k=2)$ instance considered previously we get $\epsilon'_2 \le 0.013\%$.\smallskip

\subsubsection{How to select $u$?}

For a fixed $k$, $\epsilon'_k$ decreases as $u$ grows. For a fixed $u$, $\epsilon'_k$ also decreases as $k$ grows. Transmission, however, grows with both $u$ (bigger digests) and $k$ (more iterations). We denote for the sake of precision: $\epsilon'_k = \epsilon'_{k,u,\eta}$.\smallskip
 
Fix $\eta$ and note that the number of bits transmitted per iteration ($\simeq 3ut$), is proportional to $u$. This yields an expected transmission complexity $T_{u,\eta}$:\smallskip

\[T_{u,\eta} \propto \tau = u \sum_{k=1}^{\infty} k \cdot \epsilon'_{k,u,\eta}=
\frac{u \eta(\eta-1)}{2^u} \sum_{k=1}^{\infty} k \left( \frac{1}{2^u} + \frac{(\eta-2)^2}{2^{2u}} \right)^{k-1}=
\frac{u \eta(\eta-1) 2^{3u}}{(2^u-2^{2u}+(\eta-2)^2)^2}\]

Dropping the proportionality factor $\eta(\eta-1)$, neglecting $2^u \ll 2^{2u}$ and approximating $(\eta-2)\simeq\eta$, we can optimize the function:

\[
\phi_\eta(u)=\frac{u \cdot 2^{3u}}{(2^{2u}-\eta^2)^2} 
\]

It turns out that $\phi_{10^6}(u)$ reaches its optimum for $u=19$.

\subsubsection{Note:} The previous bounds are rather crude, in particular:

\begin{itemize}
\item We consider $u$-bit prime digests while $u$-bit strings contain only about $2^u/u$ primes.\smallskip

\item In our probability calculations $\eta$ can be replaced by the total number of differences $t$. It is reasonable to assume that in most {\sl practical} settings $t \ll \eta$, but extreme instances where $t\sim\eta$ may exist as well.\smallskip

\item We used a fixed $u$ in all rounds. Nothing forbids using a different $u_k$ at each iteration\footnote{...or even fine-tuning the $u_k$ adaptively, as a function of the laundry's effect on the progressively reconciliated multisets.}.
\end{itemize}

\section{Theoretical Complexity and Algorithmic Improvements}

In this section, we analyse the theoretical costs of our algorithms and propose some algorithmic improvements.

TODO(amarilli): we should compare the time complexity to that of the other
paper, and, if we are better, insist on it. If we can combine this to
``Practical set reconciliation'', we should.

\subsection{Theoretical complexity}

Let $M(k)$ be the time required to multiply two numbers of $k$ bits.
We suppose $M(k+k') \ge M(k) + M(k')$, for any $k,k'$.
We know that the division and the modular reduction of two numbers of $k$ bits modulo a number of $k$ bits costs $\Oapp(M(k))$ \cite{burnikel1998fast}.
%The gcd computation also costs $\Oapp(M(k))$~\cite{moller2008schonhage}.
Furthermore, using naive algorithms, $M(k) = O(k^2)$, but using fast algorithms such as FFT~\cite{schonhage1971schnelle}, $M(k) = \Oapp(k)$.
We note that the FFT multiplication is faster than the other methods (naive or Karatsuba) for number of about $10^4 \cdot 64$ bits (from gmp sources -- if you find any better sources, it would be interesting...).
And using such big numbers, the division and the modulo reduction algorithms used in gmp are also the ones with complexity $\Oapp(M(k))$.

Since $p$ has $2 u t$ bits, here are the costs:
\begin{enumerate}
\item (Neil) computation of the redundancy $c=\prod_{i=1}^n h_i \bmod p$, cost: $O(n M(u t))$, $\Oapp(n u t)$ with FFT
\item (Oscar) computation of the redundancy $c'=\prod_{i=1}^n h_i \bmod p$, cost: $O(n M(u t))$, $\Oapp(n u t)$ with FFT
\item (Oscar) computation of $s = c' / c \bmod p$, cost: $M(u t)$, $\Oapp(u t)$ with FFT
\item (Oscar) computation of the two $u t$-bits number $a$ and $b$, such that $s = a / b \bmod p$, cost: $\Oapp(M(u t))$, using a new technique of Wang and Pan in \cite{pan2004rational} and \cite{wang2003acceleration}; however using naive extended gcd, it costs $\Oapp((u t)^2)$.
@fbenhamo TODO However I do not know any software where it is implemented, nor the actual speed in practice, neither if this can be adapted for the polynomial case (this can be an advantage over the polynomial method for set reconciliation -- but I think this is not the case, unfortunately, I have not access to interesting articles about polynomial rational reconstruction - but see p.139 of http://algo.inria.fr/chyzak/mpri/poly-20120112.pdf).
\item (Oscar) factorization of $a$, i.e., $n$ modulo reductions of $a$ by a $h_i$, cost: $\Oapp(n M(u t))$, $\Oapp(n u t)$ with FFT
\item (Oscar) factorization of $b$, i.e., $n$ modulo reductions of $b$ by a $h_i$, cost: $\Oapp(n M(u t))$. $\Oapp(n u t)$ with FFT
\end{enumerate}

\subsection{Improvements}

It is possible to improve the complexity of the computation of the redundancy and the factorization to $\Oapp(n/t M(u t)$, $\Oapp(n u)$ with FFT~\cite{schonhage1971schnelle}.
To simplify the explanations, let us suppose $t$ is a power of $2$: $t=2^\tau$, and $t$ divides $n$.

The idea is the following: we group $h_i$ by group of $t$ elements and we compute the product of each of these groups (without modulo)
\[ H_j = \prod_{i=j t}^{j t + t - 1} h_i. \]
Each of these products can be computed in $\Oapp(M(u t))$ using a standard method of product tree, depicted in Algorithm~\ref{alg:prod-tree} (for $j=0$) and in Figure~\ref{fig:prod-tree}.
And all these $n / t$ products can be computed in $\Oapp(n/t M(u t))$.
Then, one can compute $c$ by multiplying these products $H_j$ together, modulo $p$, which costs $\Oapp(n/t M(u t))$.

\begin{figure}[t]
\centering
\centerline{
\begin{turn}{90}
\begin{tikzpicture}[level/.style={sibling distance=60mm/#1}]
\node (z){$\displaystyle \pi=\pi_1=\prod_{i=0}^{t-1} h_i$}
  child {node (a) {$\displaystyle \pi_2=\prod_{i=0}^{t/2-1} h_i$}
    child {node (b) {$\displaystyle \pi_4=\prod_{i=0}^{t/4-1} h_i$}
      child {node {$\vdots$}
        child {node (d) {$\displaystyle \pi_t=h_0$}}
        child {node (e) {$\displaystyle \pi_{t+1}=h_1$}}
      }
      child {node {$\vdots$}}
    }
    child {node (g) {$\displaystyle \pi_5=\prod_{i=t/4}^{t/2-1} h_i$}
      child {node {$\vdots$}}
      child {node {$\vdots$}}
    }
  }
  child {node (j) {$\displaystyle \pi_3=\prod_{i=t/2}^{t-1} h_i$}
    child {node (k) {$\displaystyle \pi_6=\prod_{i=t/2}^{3t/4-1} h_i$}
      child {node {$\vdots$}}
      child {node {$\vdots$}}
    }
    child {node (l) {$\displaystyle \pi_7=\prod_{i=3t/4}^{t-1} h_i$}
      child {node {$\vdots$}}
      child {node (c){$\vdots$}
        child {node (o) {$\displaystyle h_{t-2}$}}
        child {node (p) {$\displaystyle \pi_{2t-1}=h_{t-1}$}
%
%
          child [grow=right] {node (qe) {} edge from parent[draw=none]
            child [grow=right] {node (q) {$2^\tau M(u) \le M(u t)$} edge from parent[draw=none]
            child [grow=up] {node (r) {$\vdots$} edge from parent[draw=none]
            child [grow=up] {node (s) {$4 M(u t/4) \le M(u t)$} edge from parent[draw=none]
            child [grow=up] {node (t) {$2 M(u t/2) \le M(u t)$} edge from parent[draw=none]
            child [grow=up] {node (u) {$M(u t)$} edge from parent[draw=none]}
          }}}
          child [grow=down] {node (v) {$\tau M(u t) = \Oapp(M(u t))$}edge from parent[draw=none]}
            }
          }
        }
    }
  }
};
%\path (o) -- (e) node (x) [midway] {$\cdots$}
%  child [grow=down] {
%    node (y) {$O\left(\displaystyle\sum_{i = 0}^k 2^i \cdot \frac{n}{2^i}\right)$}
%    edge from parent[draw=none]
%  };
\path (q) -- (r) node [midway] {+};
\path (s) -- (r) node [midway] {+};
\path (s) -- (t) node [midway] {+};
\path (s) -- (l) node [midway] {$\displaystyle \longrightarrow$};
\path (t) -- (u) node [midway] {+};
\path (z) -- (u) node [midway] {$\displaystyle \longrightarrow$};
\path (j) -- (t) node [midway] {$\displaystyle \longrightarrow$};
\path (p) -- (q) node [midway] {$\displaystyle \longrightarrow$};
%\path (y) -- (x) node [midway] {$\Downarrow$};
%\path (v) -- (y)
%  node (w) [midway] {$\tau M(u t) = \Oapp(M(u t))$};
\path (q) -- (v) node [midway] {$\displaystyle \le$};
%\path (e) -- (x) node [midway] {+};
%\path (o) -- (x) node [midway] {+};
%\path (y) -- (w) node [midway] {$\displaystyle \longrightarrow$};
%\path (v) -- (w) node [midway] {$\Leftrightarrow$};
%\path (r) -- (c) node [midway] {$\cdots$};
\end{tikzpicture}
\end{turn}{30}
}
\caption{Product tree}\label{fig:prod-tree}
\end{figure}



\begin{algorithm}
\newcommand{\vstart}{\ensuremath{\mathrm{start}}}
\newcommand{\vmid}{\ensuremath{\mathrm{mid}}}
\newcommand{\vend}{\ensuremath{\mathrm{end}}}
\begin{algorithmic}[1]
\Require{a table $h$ such that $h[i] = h_i$}
\Ensure{$\pi = \pi_1 = \prod_0^{t-1} h_i$, and $\pi[i] = \pi_i$ for $i \in \{1,\dots,2t-1\}$ as in Figure~\ref{fig:prod-tree}}
\State $\pi \gets $ array of size $t$
\Function{prodTree}{$i$,$\vstart$,$\vend$}
  \If{$\vstart = \vend$}
    \State \Return $1$
  \ElsIf{$\vstart+1 = \vend$}
    \State \Return $h[\vstart]$
  \Else
    \State $\vmid \gets \lfloor (\vstart+\vend)/2 \rfloor$
    \State $\pi[i] \gets $\Call{prodTree}{$2\times i$,$\vstart$,$\vmid$}
    \State $\pi[i+1] \gets $\Call{prodTree}{$2\times i+1$,$\vstart$,$\vmid$}
    \State \Return  $\times$ \Call{prodTree}{$\vmid$,$\vend$}
  \EndIf
\EndFunction
\State $\pi[1] \gets $\Call{prodTree}{$1,0,t$}
\end{algorithmic}
\caption{Product tree algorithm}\label{alg:prod-tree}
\end{algorithm}

The same technique applies for the factorization, but this time, we have to be a little more careful.
After computing the tree product, we can compute the residues of $a$ (or $b$) modulo $H_j$, then we can compute the residues of these new elements modulo the two children of $H_j$ in the product tree ($\prod_{i=j t}^{j t + t/2 - 1} h_i$ and $\prod_{i=j t}^{j t + t/2 - 1} h_i$), and then compute the residues of these two new values modulo the children of the previous children, and so on.
Intuitively, we go down the product tree doing modulo reduction.
At the end (i.e., at the leaves), we obtain the residues of $a$ modulo each of the $h_i$.
This algorithm is depicted in Algorithm~\ref{fig:div-prod-tree} and in Figure~\ref{fig:div-prod-tree} (for $j=1$).
The complexity of the algorithm is $\Oapp(M(u t))$, for each $j$.
So the total complexity is $\Oapp(n/t \Oapp(M(u t))$.

\begin{figure}[t]
\centering
\centerline{
\begin{tikzpicture}[level/.style={sibling distance=60mm/#1}]
\node (z){$\displaystyle a \bmod \pi_1$}
  child {node (a) {$\displaystyle a \bmod \pi_2$}
    child {node (b) {$\displaystyle a \bmod \pi_3$}
      child {node {$\vdots$}
        child {node (d) {$\displaystyle a \bmod h_{0}$}}
        child {node (e) {$\displaystyle a \bmod h_{1}$}}
      }
      child {node {$\vdots$}}
    }
    child {node (g) {$\displaystyle a \bmod \pi_5$}
      child {node {$\vdots$}}
      child {node {$\vdots$}}
    }
  }
  child {node (j) {$\displaystyle a \bmod \pi_3$}
    child {node (k) {$\displaystyle a \bmod \pi_6$}
      child {node {$\vdots$}}
      child {node {$\vdots$}}
    }
    child {node (l) {$\displaystyle a \bmod \pi_7$}
      child {node {$\vdots$}}
      child {node (c){$\vdots$}
        child {node (o) {$ $}}
        child {node (p) {$\displaystyle a \bmod h_{t-1}$}
%
%
          child [grow=right] {node (qe) {} edge from parent[draw=none]
            child [grow=right] {node (q) {$2^\tau O(M(u)) = O(M(u t))$} edge from parent[draw=none]
            child [grow=up] {node (r) {$\vdots$} edge from parent[draw=none]
            child [grow=up] {node (s) {$4 O(M(u t/4)) = O(M(u t))$} edge from parent[draw=none]
            child [grow=up] {node (t) {$2 O(M(u t/2)) = O(M(u t))$} edge from parent[draw=none]
            child [grow=up] {node (u) {$O(M(u t))$} edge from parent[draw=none]}
          }}}
          child [grow=down] {node (v) {$\tau O(M(u t)) = \Oapp(M(u t))$}edge from parent[draw=none]}
            }
          }
        }
    }
  }
};
%\path (o) -- (e) node (x) [midway] {$\cdots$}
%  child [grow=down] {
%    node (y) {$O\left(\displaystyle\sum_{i = 0}^k 2^i \cdot \frac{n}{2^i}\right)$}
%    edge from parent[draw=none]
%  };
\path (q) -- (r) node [midway] {+};
\path (s) -- (r) node [midway] {+};
\path (s) -- (t) node [midway] {+};
\path (s) -- (l) node [midway] {$\displaystyle \longrightarrow$};
\path (t) -- (u) node [midway] {+};
\path (z) -- (u) node [midway] {$\displaystyle \longrightarrow$};
\path (j) -- (t) node [midway] {$\displaystyle \longrightarrow$};
\path (p) -- (q) node [midway] {$\displaystyle \longrightarrow$};
%\path (y) -- (x) node [midway] {$\Downarrow$};
%\path (v) -- (y)
%  node (w) [midway] {$\tau M(u t) = \Oapp(M(u t))$};
\path (q) -- (v) node [midway] {$\displaystyle =$};
%\path (e) -- (x) node [midway] {+};
%\path (o) -- (x) node [midway] {+};
%\path (y) -- (w) node [midway] {$\displaystyle \longrightarrow$};
%\path (v) -- (w) node [midway] {$\Leftrightarrow$};
%\path (r) -- (c) node [midway] {$\cdots$};
\end{tikzpicture}}
\caption{Division from product tree}\label{fig:div-prod-tree}
\end{figure}


\begin{algorithm}
\newcommand{\vstart}{\ensuremath{\mathrm{start}}}
\newcommand{\vmid}{\ensuremath{\mathrm{mid}}}
\newcommand{\vend}{\ensuremath{\mathrm{end}}}
\begin{algorithmic}[1]
\Require{$a$ an integer, $\pi$ the product tree from Algorithm~\ref{alg:prod-tree}}
\Ensure{$A_i = A[i] = a \bmod \pi_i$ for $i \in \{1,\dots,2t-1\}$, computed as in Figure~\ref{alg:div-prod-tree}}
\State $A \gets $ array of size $t$
\Function{modTree}{$i$}
  \If{$i < 2t$}
    \State $A[i] \gets A[\lfloor i/2 \rfloor] \bmod \pi[i]$
    \State \Call{modTree}{$2 \times i$}
    \State \Call{modTree}{$2 \times i+1$}
  \EndIf
\EndFunction
\State $A[1] \gets a \bmod \pi[1]$
\State \Call{modTree}{$2$}
\State \Call{modTree}{$3$}
\end{algorithmic}
\caption{Division using product tree}\label{alg:div-prod-tree}
\end{algorithm}

\section{Optimizing Parameters}

The proposed process lends itself to a final fine-tuning. We list here some of the proposed research directions that could be investigated to that end:

\subsection{Using a Smooth $p$}

Comme explique dans un ancien email, je pense que l'on devrait utiliser un produit de petits nombres premiers au lieu d'un grand nombre premier $p$. Des l'instant que ces petits nombres premiers sont plus grands que les hashes, cela fonctionne. L'interêt est que l'on peut travailler modulo ces "petits nombres premiers" avec le CRT. Et en plus, la generation de ce modulo $p$ (pas premier) est beaucoup plus rapide.

Faster hashing into the primes using $h-h \bmod \pi + k \pi$ where $\pi$ is a product of small primes

\section{Implementation}

To illustrate the concept, the authors has coded and evaluated the proof of concept described in this section.\smallskip

The executable and source codes of the program, called {\sf btrsync}, can be downloaded from: \url{https://github.com/RobinMorisset/Btrsync}.\smallskip

The synchronisation is unidirectional (clearer). The program consist in two subprograms: a bash script and a python script:

\subsection{The Bash Script}

A bash script runs a python script (describe below) on the two computers to be synchronized. If the computer is not the one running the bash script, the python script is executed through ssh. The bash scripts also creates two pipes: one from Neil stdin to Oscar stdout and one from Oscar stdin to Neil stdout. Data exchanged during the protocol transits {\sl via} these two pipes.

\subsection{The Python Script}

The python script uses gmp which implements all the number theory operations required by Oscar and Neil, and does the actual synchronization. This script works in two phases:

\subsubsection{Finding Different Files}

\begin{enumerate}
\item Compute the hashes of all files concatenated with thier paths, type (folder/file), and permissions (not supported yet).
\item Implement the protocol proposed in Section \ref{} [add here a reference to the appropriate section in the paper] with input data comeing from stdin and output data going to stdout.
\end{enumerate}

More precisely:
\begin{itemize}
\item Oscar sends it product of hashes modulo a first prime number $p_1$.
\item Neil receives the product, divides by its own product of hashes, reconstructs the fraction modulo $p_1$ [can we elaborate more on what happens here? which functions in GMP are used to do the reconstruction?] and checks if he can factor the denominator using his hashes base. If he can, he stops and sends the numerator and the list of tuples (path, type, hash of content of the file) corresponding to the denominator's factors. Otherwise he sends "None" [is this the ASCII string "None"? if not what does he send precisely?].
\item If Neil sent "None", Oscar computes the product of hashes modulo another prime $p_2$, sends it... CRT mechanism... [can we elaborate more on what happens here? which functions in GMP are used to do the CRT?]
\item If Neil sent the numerator and a list of tuples, then Oscar factors the numerator over his own hash values. Now each party (Neil, Oscar) knows precisely the list of files (path + type + hash of content) that differs from the over party.
\end{itemize}

[please structure the following:]\smallskip

2. synchronize all the stuff [this is not an expression we can use in a paper...]. This part is not completely optimized.\smallskip

We just remove all folders Oscar should not have and create new folders.\smallskip

Then we remove all files Oscar should not have and synchronize using rsync the last files.\smallskip

We could check for move (since we have the list of hash of contents of files) and do moves locally.\smallskip

We can even try to detect moves of complete subtrees...\smallskip


Capture the following: \smallskip


integration de tout le code d'Antoine qui permet de deplacer les fichiers du cote de Neil, et evite des synchronisations inutiles de fichiers deja presents du cote de Neil. L'algorithme est plus complexe qu'il n'en a l'air car il faut gerer les cycles de deplacements, ("a" renomme en "b" lui-meme renomme en "a")... On notera que le code Haskell ne prenait pas en compte les cycles notamment (et donc etait buggue).
D'une maniere tres amusante, on peut voir l'algo propose par Antoine comme une decomposition en composantes fortement connexes du graphe des deplacements (si on oublie le cas des repertoires – les noeuds sont les fichiers, les aretes sont les deplacements a effectuer). Et l'algorithme consiste a effectuer d'abord les deplacements des composantes filles avant ceux des composantes parentes...
On remarquera que comme chaque noeud a au plus un antecedent, on peut montrer tres facilement que les composantes fortement connexes sont des noeuds seuls ou des cycles.
Pour traiter les cycles, l'algorithme stocke un des fichier dans un repertoire temporaire.
utilisation du json pour l'envoi des messages entre Neil et Oscar (beaucoup plus sur que les eval que je faisais – cependant, cela fait perdre quelques octets vu la maniere dont j'ai code...)
TODO:
benchmarks
meilleure integration du code d'Antoine pour eviter le recalcul des hashes
gestion des fichiers identiques du cote de Neil (l'algo des deplacements ne gere que le cote d'Oscar): si deux fichiers sont identiques du cote de Neil, il est inutile de transferer les deux. C'est trivial a implementer, mais il est tard


\subsection{Experimental Comparison to \rsync}

To demonstrate the benefits of our approach, we compared our \btrsync
implementation to the standard \rsync on the following datasets:

\begin{description}
  \item[\texttt{synthetic}] A directory containing 1000 very small files
    containing the numbers from 1 to 1000.
  \item[\texttt{synthetic\_shuffled}] The result of applying a few operations to
    \texttt{synthetic}: 10 files were deleted, 10 files were renamed, and the
    contents of 10 files was changed.
  \item[\texttt{source}] A snapshot of the \btrsync source tree.
  \item[\texttt{source\_moved}] The result of renaming a big folder (several
    hundred of kilobytes) in \texttt{source}.
  \item[\texttt{firefox-13.0}] The source archive of Mozilla Firefox 13.0.
  \item[\texttt{firefox-13.0.1}] The source archive of Mozilla Firefox 13.0.1.
  \item[\texttt{empty}] An empty folder.
\end{description}

We performed the measurements with \rsync version 3.0.9 (used both
as the standalone \rsync and for the underlying call to \rsync in
\btrsync). The standalone \rsync was given the \texttt{--delete} flag to delete
existing files on Oscar which do not exist on Neil. We passed the \texttt{-I}
flag to ensure that \rsync did not cheat by looking at the file modification
times (which \btrsync does not do), and \texttt{--chmod="a=rx,u+w"} as an
attempt to disable the transfer of file permissions (which \btrsync does not
transfer). Although these settings ensure that \rsync does not need to transfer
the permissions, verbose logging suggests that it transfers them anyway, so
\rsync must lose a few bytes per file as compared to \btrsync for this reason.

Bandwidth accounting was performed with the \texttt{-v} flag for \rsync
invocations (which report the number of sent and received bytes), and by
counting the amount of data transmitted for \btrsync's own negociations. The
experiment was performed between two remote hosts on a high-speed link. The time
measurements account both for CPU time and for transfer time and are just given
as a general indication.

The results are given in table~\ref{tab:results}. The timing results show that
\btrsync is slower than \rsync, especially when the number of files is high
(i.e., the synthetic datasets). The bandwidth results, however, are more
satisfactory. It is true that the trivial datasets where either Oscar or Neil
have no data, \rsync outperforms \btrsync: this is especially clear in the case
where Neil has no data: \rsync immediately notices that there is nothing to
transfer, whereas \btrsync transfers information to determine the symmetric
difference. On the non-trivial datasets, however, \btrsync outperforms \rsync.
This is the case on the synthetic datasets, where \btrsync does not have to
transfer information about all the files which were not modified, and even more
so on the case where there are no modifications at all. On the Firefox source
code dataset, \btrsync saves a very small amount of bandwidth, presumably
because of unmodified files. For the \btrsync source code dataset, we notice
that \btrsync, unlike \rsync, was able to detect the move and to avoid
retransferring the moved folder.

\begin{figure}
  \begin{tabularx}{\textwidth}{|X|X||c c|c c|c c|c c|}
    \hline
    \multicolumn{2}{|c||}{\bf Datasets} &
    \multicolumn{6}{c|}{\bf Bandwidth (bytes)} &
    \multicolumn{2}{c|}{\bf Time (s)} \\
    \hline {\bf \hfill Neil \hfill \null} & {\bf \hfill Oscar \hfill \null}
    & {\bf TX$_r$} & {\bf RX$_r$} & {\bf TX$_b$}
    & {\bf RX$_b$} & {\bf abs} & {\bf rel} & {\bf t$_r$} & {\bf t$_b$} \\

    \hline & &&&&&&&&\\[-1em]


\texttt{source} & \texttt{empty} & 1613 & 778353 & 1846 & 788357 & 10237 & +2 \% & 0.2 & 7.7 \\
\texttt{empty} & \texttt{source} & 11 & 29 & 12436 & 6120 & 18516 & +46305 \% & 0.1 & 5.5 \\
\texttt{empty} & \texttt{empty} & 11 & 29 & 19 & 28 & 7 & +32 \% & 0.1 & 0.3 \\
\texttt{synthetic} & \texttt{synthetic\_shuffled} & 24891 & 51019 & 3638 & 4147 & -68125 & -57 \% & 0.2 & 26.8 \\
\texttt{synthetic\_shuffled} & \texttt{synthetic} & 24701 & 50625 & 3443 & 3477 & -68406 & -58 \% & 0.2 & 26.6 \\
\texttt{synthetic} & \texttt{synthetic} & 25011 & 50918 & 327 & 28 & -75574 & -67 \% & 0.1 & 25.7 \\
\texttt{firefox-13.0.1} & \texttt{firefox-13.0} & 90598 & 28003573 & 80895 & 27995969 & -17307 & +0 \% & 2.6 & 4.2 \\
\texttt{source\_moved} & \texttt{source} & 2456 & 694003 & 1603 & 1974 & -692882 & -99 \% & 0.2 & 2.5 \\
\hline

  \end{tabularx}
  \caption{Experimental results. The two first columns indicate the datasets,
    synchronization is performed \emph{from} Neil \emph{to} Oscar. RX and TX are
    received and sent byte counts, $r$ and $b$ are \rsync and \btrsync, we also
    provide the absolute difference in exchanged data (positive when \btrsync
    transfers more data than \rsync) and the relative amount of data sent by
    \btrsync compared to \rsync (over $100\%$ when \btrsync transfers more data
    than $\rsync$). The last two columns show timing results.}
  \label{tab:results}
\end{figure}

\section{Haskell Implementation}

(Antoine: je range ça dans sa propre section, mais j'imagine qu'on va le couper
ou juste dire que ça existe.)

\subsection{Program Structure}

A proof-of-concept called Btrsync has been implemented in Haskell and is
available at https://github.com/RobinMorisset/Btrsync.

It is intended to work as a drop-in replacement of rsync for directories, taking
as arguments two (possibly remote) directories. It launches instances of itself
on each of these machines (by ssh), playing respectively Neil and Oscar's roles.

Communication between Neil and Oscar is handled by the original instance, that
links each agent standard output to the standard input of the other.

Niel does almost all computations, while Oscar send him the needed
informations and run the effective transfer of files when the
computations are done. Btrsync uses rsync to synchronize single files,
because it's algorithm to detect changes in a files is very good.

\subsection{Time Measurements}

Because of difficulties in linking with the GMP library the code is
significantly slower than it could be (especially in the computation of the
primes from the hashes).

TODO: benchmarks with time + bandwidth (our only benefit ..)




\section{Conclusion and Further Improvements}

In this work we [to be completed by David]\smallskip

Mention that the determination of the optimal $u$ is an interesting open question



\section{Acknowledgment}

The authors acknowledge Guillain Potron for his early involvement in this research work.\smallskip

todo: Fix euclidean to Euclidean in reference 5.\smallskip

todo: Merge two reference files rsynch and wagner.\smallskip
\nocite{rsync}
\nocite{wagner}

\bibliographystyle{splncs03}
\bibliography{btrsync}

\begin{thebibliography}{30}

\bibitem{PSRec} Y. Minsky, A. Trachtenberg, {\sl Scalable Set Reconciliation}, 40th Annual Allerton Conference on Communications, Control and Computing, Monticello, IL, October 2002. A full version entitled {\sl Practical Set Reconciliation} can be downloaded from \url{http://ipsit.bu.edu/documents/BUTR2002-01.ps}

\bibitem{Mins1} Y. Minsky, A. Trachtenberg, R. Zippel, (2003). {\sl Set reconciliation with nearly optimal communication complexity}. IEEE Transactions on Information Theory, 49(9), pp. 2213–2218.

\bibitem{comparing} Julien Cathalo, David Naccache, Jean-Jacques Quisquater, Comparing with RSA, Cryptography and Coding, Lecture Notes in Computer Science Volume 5921, 2009, pp 326-335, Cryptography and Coding, 12th IMA International Conference, Cryptography and Coding 2009, Cirencester, UK, December 15-17, 2009. Proceedings. Springer-Verlag Berlin Heidelberg

\bibitem{Whats} D. Eppstein, M. Goodrich, F. Uyeda, G. Varghese What's the difference?: efficient set reconciliation without prior context
ACM SIGCOMM Computer Communication Review - SIGCOMM '11, 41(4), pp. 218-229, 2011.


\end{thebibliography}

\appendix

\section{Extended Protocol}

\begin{center}
\begin{tabular}{|lcl|}\hline
\multicolumn{3}{|c|}{{\sf First phase during which Neil amasses modular information on the difference~~}} \\\hline
~~{\bf Oscar}                      &                                                      &   {\bf Neil}~\\
                                   &                                                      &start protocol with $p_1$~\\
                                   &~~{{\LARGE $\stackrel{c_1}{\longrightarrow}$}}~~      &   \\
                                   &                                                      &computes $a,b$ using $p_1$~\\
                                   &                                                      &if $a$ factors properly then go to {\sf Final Phase}\\
                                   &                                                      &~~~~~~else perform protocol with $p_2$~~\\
                                   &~~{{\LARGE $\stackrel{c_2}{\longrightarrow}$}}~~      &   \\
                                   &                                                      &computes $c \bmod p_1 p_2=\mbox{CRT}_{p_1,p_2}(c_1,c_2)$~~\\
                                   &                                                      &computes $a,b$ using $p_1 p_2$~\\
                                   &                                                      &if $a$ factors properly then go to {\sf Final Phase}\\
                                   &                                                      &~~~~~~else perform protocol with $p_3$~~\\
                                   &~~{{\LARGE $\stackrel{c_3}{\longrightarrow}$}}~~      &   \\
                                   &                                                      &computes $c \bmod p_1 p_2 p_3=\mbox{CRT}_{p_1,p_2,p_3}(c_1,c_2,c_3)$~~\\
                                   &                                                      &computes $a,b$ using $p_1 p_2 p_3$~\\
                                   &                                                      &if $a$ factors properly then go to {\sf Final Phase}\\
                                   &                                                      &~~~~~~else perform protocol with $p_4$ ~~\\
                                   &                  $\vdots$                            & \\\hline\hline
\multicolumn{3}{|c|}{{\sf Final Phase~~}} \\\hline
                                   &                                                      & \\
                                   &                                                      &~~~~~~Let $\mathfrak{S}=\{F'_i \mbox{~s.t.~} a \bmod h'_i =0\}$~~\\
                                   &~~{\LARGE $\stackrel{\mathfrak{S},b}{\longleftarrow}$}&\\
                                   ~~deletes files s.t. $b \bmod h_i =0$&                                                      &\\
                                   ~~adds $\mathfrak{S}$ to the disk    &                                                      &\\\hline
\end{tabular}
\end{center}

Note that the parties do not need to store the $p_i$'s in full. Indeed, the $p_i$ could be subsequent primes sharing their most significant bits. This reduces storage per prime to a small corrected additive constant $ \cong \mbox{ln}(p_i) \cong \mbox{ln}(2^{2tu+2}) \cong 1.39(tu+1)$ whose storage requires essentially $\log_2(tu)$ bits.

\end{document}
